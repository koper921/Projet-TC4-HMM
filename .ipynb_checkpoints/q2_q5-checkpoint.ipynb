{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de Markov Caché du second ordre\n",
    "\n",
    "### Application à la correction de typos dans des textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros, multiply\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "UNK = \"<unk>\"  # token to map all out-of-vocabulary words (OOVs)\n",
    "UNKid = 0  # index for UNK\n",
    "epsilon = 1e-100\n",
    "\n",
    "# le smoothing_obs permet de ne pas avoir des probabilités d'émission/ transitions nulles ( matrice souvent sparse) , cela augmente la précision \n",
    "# https://core.ac.uk/download/pdf/62868660.pdf\n",
    "class HMM:\n",
    "    def __init__(self,\n",
    "                 state_list,\n",
    "                 observation_list,\n",
    "                 transition_proba=None,\n",
    "                 transition_proba_2=None,\n",
    "                 observation_proba=None,\n",
    "                 initial_state_proba=None,\n",
    "                 smoothing_obs=0.001):\n",
    "        \"\"\"\n",
    "            Builds a Hidden Markov Model\n",
    "            * state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "            * observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "            * transition_proba is the transition probability matrix\n",
    "                [a_ij] a_ij = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            * observation_proba is the observation probablility matrix\n",
    "                [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "            * initial_state_proba is the initial state distribution\n",
    "                [pi_i] pi_i = Pr(Y_0=q_i)\"\"\"\n",
    "\n",
    "        print (\"HMM creating with: \")\n",
    "\n",
    "        self.N = len(state_list)  # number of states\n",
    "        self.M = len(observation_list)  # number of possible emissions\n",
    "\n",
    "        print (str(self.N) + \" states\")\n",
    "        print( str(self.M) + \" observations\")\n",
    "\n",
    "        self.omega_Y = state_list\n",
    "        self.omega_X = observation_list\n",
    "\n",
    "        if transition_proba is None:\n",
    "            self.transition_proba = zeros((self.N, self.N))\n",
    "        else:\n",
    "            self.transition_proba = transition_proba\n",
    "\n",
    "        if transition_proba is None:\n",
    "            self.transition_proba_2 = zeros((self.N, self.N**2))\n",
    "        else:\n",
    "            self.transition_proba_2 = transition_proba_2\n",
    "\n",
    "        if observation_proba is None:\n",
    "            self.observation_proba = zeros((self.M, self.N))\n",
    "        else:\n",
    "            self.observation_proba = observation_proba\n",
    "\n",
    "        if initial_state_proba is None:\n",
    "            self.initial_state_proba = zeros((self.N, ))\n",
    "        else:\n",
    "            self.initial_state_proba = initial_state_proba\n",
    "\n",
    "        self.make_indexes()  # build indexes, i.e the mapping between token and int\n",
    "        self.smoothing_obs = smoothing_obs\n",
    "\n",
    "    def make_indexes(self):  # OK\n",
    "        \"\"\"Creates the reverse table that maps states/observations names\n",
    "            to their index in the probabilities array\"\"\"\n",
    "        self.Y_index = {}\n",
    "        for i in range(self.N):\n",
    "            self.Y_index[self.omega_Y[i]] = i  # list d etat  // N = len(etat distinct)\n",
    "        self.X_index = {}\n",
    "        for i in range(self.M):\n",
    "            self.X_index[self.omega_X[i]] = i # omega_X = list de mot // M =len(mot distinct )\n",
    "\n",
    "    \n",
    "\n",
    "    def encode(self, sentence):  \n",
    "        \n",
    "        \"\"\" Input: sentence of tuple (word, tag)\n",
    "            return: index of word into the dictionnary\n",
    "                    of word\n",
    "                    index of tag into the dictionnary of\n",
    "                    tag\n",
    "            \"\"\"\n",
    "        word_index = []\n",
    "        tag_index = []\n",
    "        for couple in sentence:\n",
    "            mot, tag  = couple\n",
    "            if mot in self.X_index:   # self.X_index = dictionnaire { mot : indice, ...}\n",
    "                word_index.append(self.X_index[mot])\n",
    "            else:\n",
    "                word_index.append(UNKid)\n",
    "            tag_index.append(self.Y_index[tag])   # self.Y_index = dictionnaire { tag : indice, ...}\n",
    "        return word_index, tag_index   # return array indice \n",
    "\n",
    "    def observation_matrix(self, compt_pair):   \n",
    "        \"\"\" We build here the observation matrix (M, N)\n",
    "            M the number of word (into the vocabulary) , \n",
    "            N the number of states\n",
    "            Input : dictionary of count (word, tag )\n",
    "            return : Observation matrix (M,N)\n",
    "        \"\"\"\n",
    "       \n",
    "        for pair in compt_pair:\n",
    "            mot, tag = pair\n",
    "            num = compt_pair[pair]  \n",
    "            k = 0  # unkid par défault\n",
    "            if mot in self.X_index:\n",
    "                k = self.X_index[mot]    # recupère indice du mot dans dictionnaire \n",
    "            i = self.Y_index[tag]  # recupere indice du tag\n",
    "            self.observation_proba[k, i] = num\n",
    "        self.observation_proba = self.observation_proba + self.smoothing_obs\n",
    "        self.observation_proba = self.observation_proba / self.observation_proba.sum(\n",
    "            axis=0).reshape(1, self.N)\n",
    "        #return observation_proba\n",
    "\n",
    "    def transition_matrix(self, trans_counts):  # transition tag simple count (tag1, tag2)   #transition_matrix\n",
    "        \"\"\" We build here the transition matrix (N, N) \n",
    "            N the number of states\n",
    "            Input : dictionary of count (tag, tag ) :(states, states)\n",
    "            return: transition matrix (N,N) (states_t+1,states_t)\n",
    "        \"\"\"\n",
    "        for pair in trans_counts:\n",
    "            i = self.Y_index[pair[0]]\n",
    "            j = self.Y_index[pair[1]]  \n",
    "            self.transition_proba[j, i] = trans_counts[pair]     \n",
    "        self.transition_proba = self.transition_proba + self.smoothing_obs\n",
    "        self.transition_proba = self.transition_proba / self.transition_proba.sum(\n",
    "            axis=0).reshape(1, self.N)\n",
    "\n",
    "    def transition_matrix_2(self, trans_counts_2): # TRANSITION 2 \n",
    "        \n",
    "        \"\"\" We build here the transition matrix 2  (N, N**2) \n",
    "            N the number of states\n",
    "            N**2 number of tuple of states : ((a,a), (a,b)...(z,z))\n",
    "            Input : dictionary of count ((tag_2, tag_1), tag ) :((states_2, states_1),states)\n",
    "            return: transition matrix 2 (N,N**2) (states_t,(tuples (states_2,states_1)))\n",
    "        \"\"\"\n",
    "        \n",
    "        # trans_counts_2 : {(('t', 'h'), 'e'): 2479, (('h', 'e'), 'i'): 163, (('e', 'i'), 'r'): 162,\n",
    "        \n",
    "        for pair in trans_counts_2:\n",
    "            i = self.Y_index[pair[0][0]]\n",
    "            j = self.Y_index[pair[0][1]] # Y_index dictionnaire { etat: indice , ....}\n",
    "            \n",
    "            # k la ligne pour le dernier                                 \n",
    "            k = self.Y_index[pair[1]]    # en dessous : modulo 26 pour le premier i et +- 26 pour j :(a,a), (a, b )...\n",
    "            self.transition_proba_2[k, (i * self.N + j)] = trans_counts_2[pair]\n",
    "        self.transition_proba_2 = self.transition_proba_2 + self.smoothing_obs\n",
    "        self.transition_proba_2 = self.transition_proba_2 / self.transition_proba_2.sum(\n",
    "            axis=0).reshape(1, self.N**2)\n",
    "\n",
    "    def init(self, init_counts):\n",
    "        \"\"\" We build here the inititation state matrix (N,) \n",
    "            N the number of states\n",
    "            Input : dictionary of count (tag) : (states) for the first word (observation) of each sentence\n",
    "            return: initation state matrix (N,) \n",
    "        \"\"\"\n",
    "        \n",
    "        for tag in init_counts:\n",
    "            i = self.Y_index[tag]  # recupère indice tag .. \n",
    "            self.initial_state_proba[i] = init_counts[tag]\n",
    "        self.initial_state_proba = self.initial_state_proba / sum(\n",
    "            self.initial_state_proba)\n",
    "\n",
    "    def train(self, pair_counts, head_trans_counts, trans_counts, init_counts):\n",
    "        self.observation_matrix(pair_counts)\n",
    "        self.transition_matrix(head_trans_counts)\n",
    "        self.transition_matrix_2(trans_counts)\n",
    "        self.init(init_counts)\n",
    "\n",
    "    def viterbi(self, obsids):\n",
    "        \"\"\" Viterbi Algorithm : \n",
    "            Finding the most likely sequence of hidden states. \n",
    "            \"\"\"\n",
    "\n",
    "        T = len(obsids)\n",
    "\n",
    "        delta = zeros(self.N, float) \n",
    "        tmp = zeros(self.N, float)\n",
    "        psi = zeros((T, self.N), int)\n",
    "\n",
    "        delta_t = zeros(self.N, float)\n",
    "\n",
    "        delta = self.observation_proba[obsids[0]] * self.initial_state_proba\n",
    "        \n",
    "        for t in range(1, T):\n",
    "            if t == 1:\n",
    "                for i in range(self.N):\n",
    "                    for j in range(self.N):\n",
    "                        # début transition 1 \n",
    "                        tmp[j] = delta[j] * self.transition_proba[i, j]\n",
    "                    psi[t, i] = tmp.argmax()\n",
    "                    delta_t[i] = tmp.max() * self.observation_proba[obsids[t],\n",
    "                                                                    i]\n",
    "            else:\n",
    "                for i in range(self.N):\n",
    "                    for j in range(self.N):\n",
    "                        # suite => transition 2\n",
    "                        tmp[j] = delta[j] * self.transition_proba_2[\n",
    "                            i, psi[t - 1, j] * self.N + j]\n",
    "                    psi[t, i] = tmp.argmax()\n",
    "                    delta_t[i] = tmp.max() * self.observation_proba[obsids[t],\n",
    "                                                                    i]\n",
    "\n",
    "            delta, delta_t = delta_t, delta\n",
    "\n",
    "        # Chemin inverse\n",
    "        i_star = [delta.argmax()]\n",
    "        for psi_t in psi[-1:0:-1]:\n",
    "            i_star.append(psi_t[i_star[-1]])\n",
    "        i_star.reverse()\n",
    "\n",
    "        return i_star\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compter les mots et les tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fonction qui compte mots, tags, pairS (mot, tag), les transitions(tag1, tag2), les premiers mots \n",
    "# compter : lettre (x ) , lettre (y), (pair: (x, y) (lettre, lettre ), (etat, etat): (lettre, lettre ), (lettre premiere:  etat)\n",
    "\n",
    "def compteur_dict(data_sent):\n",
    "    \n",
    "    compt_mot = {}\n",
    "    compt_tag = {}\n",
    "    compt_pair= {}\n",
    "    compt_transition1 = {}\n",
    "    compt_init = {}\n",
    "    compt_transition2 = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    for sentence in data_sent:\n",
    "        for i in range(len(sentence)):\n",
    "            couple = sentence[i]\n",
    "            mot, tag = couple\n",
    "           \n",
    "            if mot in compt_mot:\n",
    "                compt_mot[mot] = compt_mot[mot] + 1\n",
    "            else:\n",
    "                compt_mot[mot] = 1\n",
    "                \n",
    "            if tag in compt_tag:\n",
    "                compt_tag[tag] = compt_tag[tag] + 1\n",
    "            else:\n",
    "                compt_tag[tag] = 1\n",
    "                \n",
    "            if couple in compt_pair:\n",
    "                compt_pair[couple] = compt_pair[couple] + 1\n",
    "            else:\n",
    "                compt_pair[couple] = 1\n",
    "            \n",
    "            if i > 1:\n",
    "                trans = ((sentence[i-2][1], sentence[i-1][1]), tag) \n",
    "                if trans in compt_transition2:\n",
    "                    compt_transition2[trans] = compt_transition2[trans] + 1\n",
    "                else:\n",
    "                    compt_transition2[trans] = 1\n",
    "                    \n",
    "            elif i == 1:\n",
    "                trans = (sentence[i-1][1], tag)\n",
    "                if trans in compt_transition1:\n",
    "                    compt_transition1[trans] = compt_transition1[trans] + 1\n",
    "                else:\n",
    "                    compt_transition1[trans] = 1\n",
    "                    \n",
    "            else:\n",
    "                if tag in compt_init:\n",
    "                    compt_init[tag] = compt_init[tag] + 1\n",
    "                else:\n",
    "                    compt_init[tag] = 1\n",
    "                    \n",
    "    return compt_mot, compt_tag, compt_pair, compt_transition1, compt_transition2, compt_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vocab(c, threshold):\n",
    "    voc = []\n",
    "    voc.append(UNK)\n",
    "    for w in c:\n",
    "        if c[w] >= threshold:\n",
    "            voc.append(w)\n",
    "    return voc\n",
    "\n",
    "def do_nothing(data_test):\n",
    "    acc= 0\n",
    "    total= 0\n",
    "    for mot in data_test:\n",
    "        for l1, l2 in mot:\n",
    "            if l1==l2:\n",
    "                acc= acc+1\n",
    "            total= total+1\n",
    "    return 1- acc/total\n",
    "# un peu moin de 3% d'amelioration avec un viterbi du 1er ordre \n",
    "\n",
    "def precision(hmm, data_test) : \n",
    "    total = 0\n",
    "    acc = 0\n",
    "\n",
    "    for mot in data_test:\n",
    "\n",
    "        obs_index, stat_index = hmm.encode(mot)\n",
    "        sequence_pred = hmm.viterbi(obs_index)\n",
    "        \n",
    "        for i in range(len(sequence_pred)) : \n",
    "            if stat_index[i]== sequence_pred[i]:\n",
    "                acc = acc +1 \n",
    "            total = total+1\n",
    "    return 1- acc/total\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('train10.pkl', 'rb') as f:\n",
    "    data_train_10 = pickle.load(f)\n",
    "    \n",
    "with open('test10.pkl', 'rb') as f:\n",
    "    data_test_10 = pickle.load(f)\n",
    "    \n",
    "with open('train20.pkl', 'rb') as f:\n",
    "    data_train_20 = pickle.load(f)\n",
    "    \n",
    "with open('test20.pkl', 'rb') as f:\n",
    "    data_test_20 = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lettres dans le train :  26\n",
      "Nombre de tags dans le train   :  26\n",
      "Nombre de paires dans le train :  127\n",
      "Nombre de transitions dans le train :  2489\n",
      "Nombre de inititiation dans le train  :  25\n",
      "{'b': 2070, 'y': 2985, 't': 13877, 'h': 6683, 'e': 18091, 'i': 10976, 'r': 8247, 'o': 11935, 'w': 2229, 'n': 9778, 'a': 10560, 'c': 4808, 'u': 3931, 'v': 1927, 'l': 6417, 's': 9762, 'f': 3379, 'm': 3773, 'd': 4541, 'g': 2736, 'k': 590, 'p': 3217, 'z': 124, 'j': 108, 'x': 274, 'q': 150}\n",
      "Vocabulaire : 27\n"
     ]
    }
   ],
   "source": [
    "compt_mot,compt_tag,compt_pair,compt_trans1,compt_trans2,compt_init = compteur_dict(data_train_10)\n",
    "print (\"Nombre de lettres dans le train : \", len(compt_mot))\n",
    "print (\"Nombre de tags dans le train   : \", len(compt_tag))\n",
    "print (\"Nombre de paires dans le train : \",len(compt_pair))\n",
    "print (\"Nombre de transitions dans le train : \" , len(compt_trans2) )\n",
    "print (\"Nombre de inititiation dans le train  : \", len(compt_init))\n",
    "print (compt_tag)\n",
    "vocab = make_vocab(compt_mot,10)\n",
    "print (\"Vocabulaire :\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du HMM et apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "27 states\n",
      "27 observations\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(state_list=vocab, \n",
    "          observation_list=vocab,\n",
    "          smoothing_obs = 0.01)\n",
    "\n",
    "hmm.train(compt_pair,compt_trans1,compt_trans2,compt_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultat test10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taux d'erreur en touchant à rien  0.10177595628415304\n",
      "taux d'erreur avec viterbi d'ordre 2  0.046038251366120164\n"
     ]
    }
   ],
   "source": [
    "print(\"taux d'erreur en touchant à rien \" , do_nothing(data_test_10))\n",
    "print(\"taux d'erreur avec viterbi d'ordre 2 \" , precision(hmm, data_test_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Resultat test20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "27 states\n",
      "27 observations\n",
      "taux d'erreur en touchant à rien  0.19405667725121323\n",
      "taux d'erreur avec viterbi d'ordre 2  0.09262476783895512\n"
     ]
    }
   ],
   "source": [
    "compt_mot,compt_tag,compt_pair,compt_trans1,compt_trans2,compt_init = compteur_dict(data_train_20)\n",
    "\n",
    "hmm = HMM(state_list=vocab, \n",
    "          observation_list=vocab, \n",
    "          smoothing_obs = 0.001)\n",
    "\n",
    "hmm.train(compt_pair,compt_trans1,compt_trans2,compt_init)\n",
    "print(\"taux d'erreur en touchant à rien \" , do_nothing(data_test_20))\n",
    "print(\"taux d'erreur avec viterbi d'ordre 2 \" , precision(hmm, data_test_20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion lettres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On cree un corpus sans erreur \n",
    "def clean_corpus(data):\n",
    "    clean_corp =[]\n",
    "    \n",
    "    for mot in data:\n",
    "        clean_mot=[]\n",
    "        for pair in mot :\n",
    "            mot, tag = pair\n",
    "            clean_mot.append((tag,tag))\n",
    "        clean_corp.append(clean_mot)\n",
    "    return clean_corp \n",
    "\n",
    "\n",
    "data_clean_train = clean_corpus(data_train_10)  \n",
    "data_clean_test = clean_corpus(data_test_10)\n",
    "do_nothing(data_clean_train)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on insere des caractères au hasard dans le corpus \n",
    "# exemple : [('a', 'a'), ('c', 'c'), ('c', 'c'), ('o', 'o'), ('u', 'u'), ('n', 'n'), ('t', 't')]\n",
    "# devient :  [('a', 'a'), ('c', 'c'), ('c', 'c'), ('w', '<ins>'), ('o', 'o'), ('u', 'u'), ('n', 'n'), ('t', 't')]\n",
    "\n",
    "def insert_carac(data, taux_erreur = 0.1):\n",
    "    import string\n",
    "    import random\n",
    "    alphabet =list(string.ascii_lowercase)\n",
    "    err = 0\n",
    "    total = 0\n",
    "    for mot in data:\n",
    "        for couple in mot:\n",
    "            total = total+1\n",
    "    taux_erreur = err / total \n",
    "    \n",
    "    while  taux_erreur <0.1:\n",
    "        i = random.randint(0,len(data)-1)\n",
    "        mot = data[i]\n",
    "        j= random.randint(0,len(mot)-1)\n",
    "        lettre_insert = alphabet[random.randint(0,len(alphabet)-1)] \n",
    "        data[i].insert(j,(lettre_insert, '<ins>'))\n",
    "        err = err + 1 \n",
    "        taux_erreur = err/total \n",
    "    return data \n",
    "\n",
    "def make_vocab_insert(c, threshold):\n",
    "   \n",
    "    voc = list()\n",
    "    voc.append(UNK)\n",
    "    voc.append('<ins>')\n",
    "    for w in c:\n",
    "        if c[w] >= threshold:\n",
    "            voc.append(w)\n",
    "    return voc\n",
    "\n",
    "\n",
    "\n",
    "data_insert_train = insert_carac(data_clean_train, taux_erreur = 0.1)\n",
    "data_insert_test = insert_carac(data_clean_test, taux_erreur = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "28 states\n",
      "28 observations\n",
      "taux d'erreur en touchant à rien  0.09090909090909094\n",
      "taux d'erreur avec viterbi d'ordre 2  0.05278191753601591\n"
     ]
    }
   ],
   "source": [
    "compt_mot,compt_tag,compt_pair,compt_trans1,compt_trans2,compt_init = compteur_dict(data_insert_train)\n",
    "\n",
    "vocab= make_vocab_insert(compt_mot, 10)\n",
    "hmm = HMM(state_list=vocab, \n",
    "          observation_list=vocab, \n",
    "          smoothing_obs = 0.001)\n",
    "\n",
    "hmm.train(compt_pair,compt_trans1,compt_trans2,compt_init)\n",
    "print(\"taux d'erreur en touchant à rien \" , do_nothing(data_insert_test))\n",
    "print(\"taux d'erreur avec viterbi d'ordre 2 \" , precision(hmm, data_insert_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('by', 'by')],\n",
       " [('th', 'th'), ('ei', 'ei'), ('r', 'r')],\n",
       " [('ow', 'ow'), ('n', 'n')],\n",
       " [('ac', 'ac'), ('co', 'co'), ('n', 'un'), ('t', 't')],\n",
       " [('i', 'vi'), ('ol', 'ol'), ('en', 'en'), ('ce', 'ce')],\n",
       " [('s', 'is')],\n",
       " [('fo', 'fo'), ('r', 'r')],\n",
       " [('th', 'th'), ('em', 'em')],\n",
       " [('a', 'a')],\n",
       " [('fo', 'fo'), ('rm', 'rm')],\n",
       " [('of', 'of')],\n",
       " [('li', 'li'), ('be', 'be'), ('ra', 'ra'), ('ti', 'ti'), ('on', 'on')],\n",
       " [('in', 'in')],\n",
       " [('ot', 'ot'), ('he', 'he'), ('r', 'r')],\n",
       " [('wo', 'wo'), ('rd', 'rd'), ('s', 's')],\n",
       " [('by', 'by')],\n",
       " [('co', 'co'), ('mm', 'mm'), ('it', 'it'), ('ti', 'ti'), ('ng', 'ng')],\n",
       " [('vi', 'vi'), ('ol', 'ol'), ('en', 'en'), ('e', 'ce')],\n",
       " [('t', 'th'), ('ey', 'ey')],\n",
       " [('br', 'br'), ('ea', 'ea'), ('k', 'k')],\n",
       " [('th', 'th'), ('ro', 'ro'), ('ug', 'ug'), ('h', 'h')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('ps', 'ps'),\n",
       "  ('yc', 'yc'),\n",
       "  ('ho', 'ho'),\n",
       "  ('lo', 'lo'),\n",
       "  ('gi', 'gi'),\n",
       "  ('ca', 'ca'),\n",
       "  ('l', 'l')],\n",
       " [('re', 're'), ('st', 'st'), ('ra', 'ra'), ('in', 'in'), ('ts', 'ts')],\n",
       " [('th', 'th'), ('at', 'at')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('be', 'be'), ('en', 'en')],\n",
       " [('tr', 'tr'), ('ai', 'ai'), ('ne', 'ne'), ('d', 'd')],\n",
       " [('in', 'in'), ('to', 'to')],\n",
       " [('th', 'th'), ('em', 'em')],\n",
       " [('be', 'be'), ('ca', 'ca'), ('us', 'us'), ('e', 'e')],\n",
       " [('th', 'th'), ('ey', 'ey')],\n",
       " [('ar', 'ar'), ('e', 'e')],\n",
       " [('ov', 'ov'),\n",
       "  ('er', 'er'),\n",
       "  ('so', 'so'),\n",
       "  ('ci', 'ci'),\n",
       "  ('al', 'al'),\n",
       "  ('iz', 'iz'),\n",
       "  ('ed', 'ed')],\n",
       " [('th', 'th'), ('es', 'es'), ('e', 'e')],\n",
       " [('re', 're'), ('st', 'st'), ('ra', 'ra'), ('in', 'in'), ('ts', 'ts')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('be', 'be'), ('en', 'en')],\n",
       " [('o', 'mo'), ('re', 're')],\n",
       " [('co', 'co'), ('nf', 'nf'), ('in', 'in'), ('in', 'in'), ('g', 'g')],\n",
       " [('fo', 'fo'), ('r', 'r')],\n",
       " [('th', 'th'), ('em', 'em')],\n",
       " [('th', 'th'), ('an', 'an')],\n",
       " [('fo', 'fo'), ('r', 'r')],\n",
       " [('ot', 'ot'), ('he', 'he'), ('rs', 'rs')],\n",
       " [('he', 'he'), ('nc', 'nc'), ('e', 'e')],\n",
       " [('th', 'th'), ('e', 'ei'), ('r', 'r')],\n",
       " [('ne', 'ne'), ('ed', 'ed')],\n",
       " [('t', 'to')],\n",
       " [('br', 'br'), ('ea', 'ea'), ('k', 'k')],\n",
       " [('fr', 'fr'), ('ee', 'ee')],\n",
       " [('f', 'of')],\n",
       " [('th', 'th'), ('em', 'em')],\n",
       " [('bu', 'bu'), ('t', 't')],\n",
       " [('th', 'th'), ('ey', 'ey')],\n",
       " [('us', 'us'), ('u', 'ua'), ('ll', 'll'), ('y', 'y')],\n",
       " [('ju', 'ju'), ('st', 'st'), ('if', 'if'), ('y', 'y')],\n",
       " [('th', 'th'), ('ei', 'ei'), ('r', 'r')],\n",
       " [('re', 're'), ('be', 'be'), ('ll', 'll'), ('io', 'io'), ('n', 'n')],\n",
       " [('in', 'in')],\n",
       " [('e', 'te'), ('rm', 'rm'), ('s', 's')],\n",
       " [('o', 'of')],\n",
       " [('ma', 'ma'), ('in', 'in'), ('st', 'st'), ('re', 're'), ('am', 'am')],\n",
       " [('va', 'va'), ('lu', 'lu'), ('es', 'es')],\n",
       " [('if', 'if')],\n",
       " [('t', 'th'), ('ey', 'ey')],\n",
       " [('en', 'en'), ('ga', 'ga'), ('ge', 'ge')],\n",
       " [('in', 'in')],\n",
       " [('vi', 'vi'), ('ol', 'ol'), ('en', 'en'), ('c', 'ce')],\n",
       " [('th', 'th'), ('e', 'ey')],\n",
       " [('cl', 'cl'), ('ai', 'ai'), ('m', 'm')],\n",
       " [('to', 'to')],\n",
       " [('be', 'be')],\n",
       " [('fi', 'fi'), ('gh', 'gh'), ('ti', 'ti'), ('ng', 'ng')],\n",
       " [('ag', 'ag'), ('ai', 'ai'), ('ns', 'ns'), ('t', 't')],\n",
       " [('ra', 'ra'), ('ci', 'ci'), ('sm', 'sm')],\n",
       " [('or', 'or')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('li', 'li'), ('ke', 'ke')],\n",
       " [('we', 'we')],\n",
       " [('re', 're'), ('al', 'al'), ('iz', 'iz'), ('e', 'e')],\n",
       " [('th', 'th'), ('at', 'at')],\n",
       " [('m', 'ma'), ('y', 'ny')],\n",
       " [('ob', 'ob'), ('j', 'je'), ('ct', 'ct'), ('io', 'io'), ('ns', 'ns')],\n",
       " [('co', 'co'), ('ul', 'ul'), ('d', 'd')],\n",
       " [('be', 'be')],\n",
       " [('ra', 'ra'), ('is', 'is'), ('ed', 'ed')],\n",
       " [('to', 'to')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('fo', 'fo'), ('re', 're'), ('go', 'go'), ('in', 'in'), ('g', 'g')],\n",
       " [('th', 'th'), ('u', 'um'), ('b', 'b')],\n",
       " [('na', 'na'), ('il', 'il')],\n",
       " [('sk', 'sk'), ('et', 'et'), ('ch', 'ch')],\n",
       " [('f', 'of')],\n",
       " [('le', 'le'), ('ft', 'ft'), ('is', 'is'), ('t', 't')],\n",
       " [('ps', 'ps'), ('yc', 'yc'), ('ho', 'ho'), ('lo', 'lo'), ('gy', 'gy')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('re', 're'), ('al', 'al')],\n",
       " [('si', 'si'), ('u', 'tu'), ('at', 'at'), ('io', 'io'), ('n', 'n')],\n",
       " [('is', 'is')],\n",
       " [('co', 'co'), ('mp', 'mp'), ('le', 'le'), ('x', 'x')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('an', 'an'), ('t', 'yt'), ('hi', 'hi'), ('ng', 'ng')],\n",
       " [('li', 'li'), ('ke', 'ke')],\n",
       " [('a', 'a')],\n",
       " [('co', 'co'), ('mp', 'mp'), ('le', 'le'), ('te', 'te')],\n",
       " [('de', 'de'),\n",
       "  ('s', 'sc'),\n",
       "  ('ri', 'ri'),\n",
       "  ('pt', 'pt'),\n",
       "  ('io', 'io'),\n",
       "  ('n', 'n')],\n",
       " [('of', 'of')],\n",
       " [('it', 'it')],\n",
       " [('wo', 'wo'), ('ul', 'ul'), ('d', 'd')],\n",
       " [('ta', 'ta'), ('k', 'ke')],\n",
       " [('se', 'se'), ('ve', 've'), ('ra', 'ra'), ('l', 'l')],\n",
       " [('vo', 'vo'), ('lu', 'lu'), ('me', 'me'), ('s', 's')],\n",
       " [('ev', 'ev'), ('en', 'en')],\n",
       " [('if', 'if')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('e', 'ne'), ('ce', 'ce'), ('ss', 'ss'), ('ar', 'ar'), ('y', 'y')],\n",
       " [('da', 'da'), ('ta', 'ta')],\n",
       " [('w', 'we'), ('re', 're')],\n",
       " [('av', 'av'), ('ai', 'ai'), ('la', 'la'), ('bl', 'bl'), ('e', 'e')],\n",
       " [('we', 'we')],\n",
       " [('cl', 'cl'), ('ai', 'ai'), ('m', 'm')],\n",
       " [('on', 'on'), ('ly', 'ly')],\n",
       " [('t', 'to')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('in', 'in'), ('di', 'di'), ('ca', 'ca'), ('te', 'te'), ('d', 'd')],\n",
       " [('ve', 've'), ('ry', 'ry')],\n",
       " [('ro', 'ro'), ('u', 'ug'), ('hl', 'hl'), ('y', 'y')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('w', 'tw'), ('o', 'o')],\n",
       " [('mo', 'mo'), ('st', 'st')],\n",
       " [('im', 'im'), ('po', 'po'), ('rt', 'rt'), ('an', 'an'), ('t', 't')],\n",
       " [('te', 'te'), ('nd', 'nd'), ('en', 'en'), ('ci', 'ci'), ('es', 'es')],\n",
       " [('in', 'in')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('ps', 'ps'), ('yc', 'yc'), ('ho', 'ho'), ('lo', 'lo'), ('gy', 'gy')],\n",
       " [('of', 'of')],\n",
       " [('mo', 'mo'), ('de', 'de'), ('n', 'rn')],\n",
       " [('le', 'le'), ('t', 'ft'), ('i', 'is'), ('m', 'm')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('p', 'pr'), ('b', 'ob'), ('l', 'le'), ('ms', 'ms')],\n",
       " [('of', 'of')],\n",
       " [('t', 'th'), ('e', 'e')],\n",
       " [('le', 'le'), ('ft', 'ft'), ('is', 'is'), ('t', 't')],\n",
       " [('a', 'ar'), ('e', 'e')],\n",
       " [('in', 'in'), ('di', 'di'), ('ca', 'ca'), ('ti', 'ti'), ('ve', 've')],\n",
       " [('of', 'of')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('pr', 'pr'), ('ob', 'ob'), ('le', 'le'), ('ms', 'ms')],\n",
       " [('of', 'of')],\n",
       " [('ou', 'ou'), ('r', 'r')],\n",
       " [('so', 'so'), ('ci', 'ci'), ('et', 'et'), ('y', 'y')],\n",
       " [('s', 'as')],\n",
       " [('a', 'a')],\n",
       " [('wh', 'wh'), ('l', 'ol'), ('e', 'e')],\n",
       " [('lo', 'lo'), ('w', 'w')],\n",
       " [('se', 'se'), ('lf', 'lf')],\n",
       " [('es', 'es'), ('te', 'te'), ('em', 'em')],\n",
       " [('de', 'de'), ('pr', 'pr'), ('es', 'es'), ('si', 'si'), ('ve', 've')],\n",
       " [('te', 'te'), ('nd', 'nd'), ('en', 'en'), ('ci', 'ci'), ('es', 'es')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('de', 'de'), ('fe', 'fe'), ('at', 'at'), ('is', 'is'), ('m', 'm')],\n",
       " [('r', 'ar'), ('e', 'e')],\n",
       " [('no', 'no'), ('t', 't')],\n",
       " [('re', 're'), ('st', 'st'), ('ri', 'ri'), ('ct', 'ct'), ('ed', 'ed')],\n",
       " [('to', 'to')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('le', 'le'), ('ft', 'ft')],\n",
       " [('t', 'th'), ('ou', 'ou'), ('gh', 'gh')],\n",
       " [('th', 'th'), ('ey', 'ey')],\n",
       " [('ar', 'ar'), ('e', 'e')],\n",
       " [('es', 'es'), ('pe', 'pe'), ('ci', 'ci'), ('al', 'al'), ('ly', 'ly')],\n",
       " [('no', 'no'), ('ti', 'ti'), ('ce', 'ce'), ('ab', 'ab'), ('le', 'le')],\n",
       " [('in', 'in')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('le', 'le'), ('ft', 'ft')],\n",
       " [('th', 'th'), ('ey', 'ey')],\n",
       " [('ar', 'ar'), ('e', 'e')],\n",
       " [('w', 'wi'), ('de', 'de'), ('sp', 'sp'), ('re', 're'), ('ad', 'ad')],\n",
       " [('in', 'in')],\n",
       " [('ou', 'ou'), ('r', 'r')],\n",
       " [('so', 'so'), ('ci', 'ci'), ('et', 'et'), ('y', 'y')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('to', 'to'), ('da', 'da'), ('y', 'y')],\n",
       " [('s', 's')],\n",
       " [('so', 'so'), ('ci', 'ci'), ('t', 'et'), ('y', 'y')],\n",
       " [('t', 'tr'), ('e', 'ie'), ('s', 's')],\n",
       " [('t', 'to')],\n",
       " [('so', 'so'), ('ci', 'ci'), ('al', 'al'), ('iz', 'iz'), ('e', 'e')],\n",
       " [('s', 'us')],\n",
       " [('to', 'to')],\n",
       " [('a', 'a')],\n",
       " [('gr', 'gr'), ('ea', 'ea'), ('te', 'te'), ('r', 'r')],\n",
       " [('ex', 'ex'), ('te', 'te'), ('nt', 'nt')],\n",
       " [('th', 'th'), ('an', 'an')],\n",
       " [('an', 'an'), ('y', 'y')],\n",
       " [('pr', 'pr'), ('ev', 'ev'), ('io', 'io'), ('us', 'us')],\n",
       " [('so', 'so'), ('ci', 'ci'), ('et', 'et'), ('y', 'y')],\n",
       " [('e', 'we')],\n",
       " [('ar', 'ar'), ('e', 'e')],\n",
       " [('ev', 'ev'), ('en', 'en')],\n",
       " [('to', 'to'), ('ld', 'ld')],\n",
       " [('by', 'by')],\n",
       " [('x', 'ex'), ('pe', 'pe'), ('rt', 'rt'), ('s', 's')],\n",
       " [('ho', 'ho'), ('w', 'w')],\n",
       " [('to', 'to')],\n",
       " [('ea', 'ea'), ('t', 't')],\n",
       " [('ho', 'ho'), ('w', 'w')],\n",
       " [('to', 'to')],\n",
       " [('ex', 'ex'), ('er', 'er'), ('ci', 'ci'), ('se', 'se')],\n",
       " [('ho', 'ho'), ('w', 'w')],\n",
       " [('to', 'to')],\n",
       " [('ma', 'ma'), ('ke', 'ke')],\n",
       " [('lo', 'lo'), ('ve', 've')],\n",
       " [('ho', 'ho'), ('w', 'w')],\n",
       " [('to', 'to')],\n",
       " [('ra', 'ra'), ('is', 'is'), ('e', 'e')],\n",
       " [('ou', 'ou'), ('r', 'r')],\n",
       " [('ki', 'ki'), ('ds', 'ds')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('so', 'so')],\n",
       " [('fo', 'fo'), ('rt', 'rt'), ('h', 'h')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('po', 'po'), ('w', 'we'), ('r', 'r')],\n",
       " [('pr', 'pr'), ('oc', 'oc'), ('es', 'es'), ('s', 's')],\n",
       " [('hu', 'hu'), ('ma', 'ma'), ('n', 'n')],\n",
       " [('be', 'be'), ('in', 'in'), ('gs', 'gs')],\n",
       " [('h', 'ha'), ('v', 've')],\n",
       " [('a', 'a')],\n",
       " [('ne', 'ne'), ('ed', 'ed')],\n",
       " [('pr', 'pr'), ('ob', 'ob'), ('ab', 'ab'), ('ly', 'ly')],\n",
       " [('ba', 'ba'), ('s', 'se'), ('d', 'd')],\n",
       " [('in', 'in')],\n",
       " [('bi', 'bi'), ('ol', 'ol'), ('og', 'og'), ('y', 'y')],\n",
       " [('fo', 'fo'), ('r', 'r')],\n",
       " [('s', 'so'), ('me', 'me'), ('th', 'th'), ('in', 'in'), ('g', 'g')],\n",
       " [('th', 'th'), ('at', 'at')],\n",
       " [('we', 'we')],\n",
       " [('wi', 'wi'), ('ll', 'll')],\n",
       " [('ca', 'ca'), ('ll', 'll')],\n",
       " [('h', 'th'), ('e', 'e')],\n",
       " [('po', 'po'), ('we', 'we'), ('r', 'r')],\n",
       " [('pr', 'pr'), ('oc', 'oc'), ('es', 'es'), ('s', 's')],\n",
       " [('h', 'th'), ('is', 'is')],\n",
       " [('is', 'is')],\n",
       " [('cl', 'cl'), ('os', 'os'), ('el', 'el'), ('y', 'y')],\n",
       " [('re', 're'), ('la', 'la'), ('te', 'te'), ('d', 'd')],\n",
       " [('to', 'to')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('e', 'ne'), ('ed', 'ed')],\n",
       " [('fo', 'fo'), ('r', 'r')],\n",
       " [('po', 'po'), ('w', 'we'), ('r', 'r')],\n",
       " [('wh', 'wh'), ('c', 'ic'), ('h', 'h')],\n",
       " [('i', 'is')],\n",
       " [('wi', 'wi'), ('de', 'de'), ('ly', 'ly')],\n",
       " [('re', 're'), ('co', 'co'), ('gn', 'gn'), ('iz', 'iz'), ('ed', 'ed')],\n",
       " [('bu', 'bu'), ('t', 't')],\n",
       " [('is', 'is')],\n",
       " [('no', 'no'), ('t', 't')],\n",
       " [('qu', 'qu'), ('it', 'it'), ('e', 'e')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('s', 'sa'), ('me', 'me')],\n",
       " [('th', 'th'), ('in', 'in'), ('g', 'g')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('p', 'po'), ('we', 'we'), ('r', 'r')],\n",
       " [('pr', 'pr'), ('oc', 'oc'), ('es', 'es'), ('s', 's')],\n",
       " [('ha', 'ha'), ('s', 's')],\n",
       " [('fo', 'fo'), ('ur', 'ur')],\n",
       " [('el', 'el'), ('em', 'em'), ('en', 'en'), ('ts', 'ts')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('th', 'th'), ('re', 're'), ('e', 'e')],\n",
       " [('mo', 'mo'), ('st', 'st')],\n",
       " [('cl', 'cl'), ('ea', 'ea'), ('r', 'r')],\n",
       " [('c', 'cu'), ('t', 't')],\n",
       " [('of', 'of')],\n",
       " [('h', 'th'), ('es', 'es'), ('e', 'e')],\n",
       " [('w', 'we')],\n",
       " [('ca', 'ca'), ('ll', 'll')],\n",
       " [('go', 'go'), ('al', 'al')],\n",
       " [('ef', 'ef'), ('fo', 'fo'), ('rt', 'rt')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('at', 'at'), ('ta', 'ta'), ('in', 'in'), ('me', 'me'), ('nt', 'nt')],\n",
       " [('f', 'of')],\n",
       " [('go', 'go'), ('l', 'al')],\n",
       " [('ev', 'ev'), ('er', 'er'), ('yo', 'yo'), ('ne', 'ne')],\n",
       " [('ne', 'ne'), ('ed', 'ed'), ('s', 's')],\n",
       " [('o', 'to')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('go', 'go'), ('al', 'al'), ('s', 's')],\n",
       " [('wh', 'wh'), ('os', 'os'), ('e', 'e')],\n",
       " [('at', 'at'), ('ta', 'ta'), ('in', 'in'), ('me', 'me'), ('nt', 'nt')],\n",
       " [('re', 're'), ('qu', 'qu'), ('ir', 'ir'), ('es', 'es')],\n",
       " [('ef', 'ef'), ('fo', 'fo'), ('rt', 'rt')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('ne', 'ne'), ('ed', 'ed'), ('s', 's')],\n",
       " [('to', 'to')],\n",
       " [('su', 'su'), ('cc', 'cc'), ('e', 'ee'), ('d', 'd')],\n",
       " [('in', 'in')],\n",
       " [('at', 'at'), ('ta', 'ta'), ('in', 'in'), ('in', 'in'), ('g', 'g')],\n",
       " [('at', 'at')],\n",
       " [('le', 'le'), ('as', 'as'), ('t', 't')],\n",
       " [('so', 'so'), ('me', 'me')],\n",
       " [('of', 'of')],\n",
       " [('hi', 'hi'), ('s', 's')],\n",
       " [('go', 'go'), ('al', 'al'), ('s', 's')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('fo', 'fo'), ('u', 'ur'), ('th', 'th')],\n",
       " [('el', 'el'), ('em', 'em'), ('en', 'en'), ('t', 't')],\n",
       " [('is', 'is')],\n",
       " [('mo', 'mo'), ('re', 're')],\n",
       " [('di', 'di'), ('ff', 'ff'), ('ic', 'ic'), ('ul', 'ul'), ('t', 't')],\n",
       " [('to', 'to')],\n",
       " [('de', 'de'), ('fi', 'fi'), ('ne', 'ne')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('ma', 'ma'), ('y', 'y')],\n",
       " [('no', 'no'), ('t', 't')],\n",
       " [('be', 'be')],\n",
       " [('ne', 'ne'), ('ce', 'ce'), ('ss', 'ss'), ('ar', 'ar'), ('y', 'y')],\n",
       " [('fo', 'fo'), ('r', 'r')],\n",
       " [('ev', 'ev'), ('er', 'er'), ('yo', 'yo'), ('ne', 'ne')],\n",
       " [('we', 'we')],\n",
       " [('ca', 'ca'), ('ll', 'll')],\n",
       " [('it', 'it')],\n",
       " [('au', 'au'), ('to', 'to'), ('no', 'no'), ('my', 'my')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('wi', 'wi'), ('l', 'll')],\n",
       " [('di', 'di'), ('sc', 'sc'), ('us', 'us'), ('s', 's')],\n",
       " [('it', 'it')],\n",
       " [('la', 'la'), ('te', 'te'), ('r', 'r')],\n",
       " [('pa', 'pa'), ('ra', 'ra'), ('gr', 'gr'), ('ap', 'ap'), ('hs', 'hs')],\n",
       " [('co', 'co'), ('ns', 'ns'), ('id', 'id'), ('er', 'er')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('hy', 'hy'),\n",
       "  ('po', 'po'),\n",
       "  ('th', 'th'),\n",
       "  ('et', 'et'),\n",
       "  ('ic', 'ic'),\n",
       "  ('al', 'al')],\n",
       " [('ca', 'ca'), ('se', 'se')],\n",
       " [('of', 'of')],\n",
       " [('a', 'a')],\n",
       " [('ma', 'ma'), ('n', 'n')],\n",
       " [('wh', 'wh'), ('o', 'o')],\n",
       " [('ca', 'ca'), ('n', 'n')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('an', 'an'), ('yt', 'yt'), ('hi', 'hi'), ('n', 'ng')],\n",
       " [('he', 'he')],\n",
       " [('wa', 'wa'), ('nt', 'nt'), ('s', 's')],\n",
       " [('ju', 'ju'), ('t', 'st')],\n",
       " [('b', 'by')],\n",
       " [('wi', 'wi'), ('sh', 'sh'), ('in', 'in'), ('g', 'g')],\n",
       " [('fo', 'fo'), ('r', 'r')],\n",
       " [('it', 'it')],\n",
       " [('su', 'su'), ('ch', 'ch')],\n",
       " [('a', 'a')],\n",
       " [('ma', 'ma'), ('n', 'n')],\n",
       " [('a', 'ha'), ('s', 's')],\n",
       " [('po', 'po'), ('we', 'we'), ('r', 'r')],\n",
       " [('b', 'bu'), ('t', 't')],\n",
       " [('h', 'he')],\n",
       " [('i', 'wi'), ('l', 'll')],\n",
       " [('de', 'de'), ('ve', 've'), ('l', 'lo'), ('p', 'p')],\n",
       " [('se', 'se'), ('ri', 'ri'), ('ou', 'ou'), ('s', 's')],\n",
       " [('ps', 'ps'),\n",
       "  ('yc', 'yc'),\n",
       "  ('ho', 'ho'),\n",
       "  ('lo', 'lo'),\n",
       "  ('gi', 'gi'),\n",
       "  ('ca', 'ca'),\n",
       "  ('l', 'l')],\n",
       " [('pr', 'pr'), ('ob', 'ob'), ('le', 'le'), ('ms', 'ms')],\n",
       " [('at', 'at')],\n",
       " [('fi', 'fi'), ('rs', 'rs'), ('t', 't')],\n",
       " [('e', 'he')],\n",
       " [('wi', 'wi'), ('ll', 'll')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('a', 'a')],\n",
       " [('lo', 'lo'), ('t', 't')],\n",
       " [('of', 'of')],\n",
       " [('fu', 'fu'), ('n', 'n')],\n",
       " [('bu', 'bu'), ('t', 't')],\n",
       " [('by', 'by')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('y', 'by')],\n",
       " [('he', 'he')],\n",
       " [('wi', 'wi'), ('ll', 'll')],\n",
       " [('be', 'be'), ('co', 'co'), ('me', 'me')],\n",
       " [('ac', 'ac'), ('ut', 'ut'), ('el', 'el'), ('y', 'y')],\n",
       " [('bo', 'bo'), ('re', 're'), ('d', 'd')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('de', 'de'),\n",
       "  ('mo', 'mo'),\n",
       "  ('ra', 'ra'),\n",
       "  ('li', 'li'),\n",
       "  ('ze', 'ze'),\n",
       "  ('d', 'd')],\n",
       " [('ev', 'ev'), ('en', 'en'), ('tu', 'tu'), ('al', 'al'), ('ly', 'ly')],\n",
       " [('he', 'he')],\n",
       " [('m', 'ma'), ('y', 'y')],\n",
       " [('be', 'be'), ('co', 'co'), ('me', 'me')],\n",
       " [('cl', 'cl'), ('in', 'in'), ('ic', 'ic'), ('al', 'al'), ('ly', 'ly')],\n",
       " [('e', 'de'), ('pr', 'pr'), ('es', 'es'), ('se', 'se'), ('d', 'd')],\n",
       " [('hi', 'hi'), ('st', 'st'), ('or', 'or'), ('y', 'y')],\n",
       " [('sh', 'sh'), ('ow', 'ow'), ('s', 's')],\n",
       " [('th', 'th'), ('at', 'at')],\n",
       " [('le', 'le'), ('is', 'is'), ('ur', 'ur'), ('ed', 'ed')],\n",
       " [('ar', 'ar'),\n",
       "  ('is', 'is'),\n",
       "  ('to', 'to'),\n",
       "  ('cr', 'cr'),\n",
       "  ('ac', 'ac'),\n",
       "  ('ie', 'ie'),\n",
       "  ('s', 's')],\n",
       " [('te', 'te'), ('nd', 'nd')],\n",
       " [('to', 'to')],\n",
       " [('b', 'be'), ('co', 'co'), ('me', 'me')],\n",
       " [('de', 'de'), ('ca', 'ca'), ('de', 'de'), ('nt', 'nt')],\n",
       " [('th', 'th'), ('is', 'is')],\n",
       " [('is', 'is')],\n",
       " [('no', 'no'), ('t', 't')],\n",
       " [('tr', 'tr'), ('ue', 'ue')],\n",
       " [('of', 'of')],\n",
       " [('fi', 'fi'), ('gh', 'gh'), ('ti', 'ti'), ('ng', 'ng')],\n",
       " [('ar', 'ar'),\n",
       "  ('is', 'is'),\n",
       "  ('to', 'to'),\n",
       "  ('cr', 'cr'),\n",
       "  ('ac', 'ac'),\n",
       "  ('i', 'ie'),\n",
       "  ('s', 's')],\n",
       " [('th', 'th'), ('at', 'at')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('o', 'to')],\n",
       " [('st', 'st'), ('ru', 'ru'), ('gg', 'gg'), ('le', 'le')],\n",
       " [('t', 'to')],\n",
       " [('ma', 'ma'), ('in', 'in'), ('ta', 'ta'), ('in', 'in')],\n",
       " [('th', 'th'), ('ei', 'ei'), ('r', 'r')],\n",
       " [('po', 'po'), ('w', 'we'), ('r', 'r')],\n",
       " [('bu', 'bu'), ('t', 't')],\n",
       " [('le', 'le'), ('is', 'is'), ('u', 'ur'), ('ed', 'ed')],\n",
       " [('se', 'se'), ('cu', 'cu'), ('re', 're')],\n",
       " [('a', 'ar'),\n",
       "  ('is', 'is'),\n",
       "  ('to', 'to'),\n",
       "  ('cr', 'cr'),\n",
       "  ('ac', 'ac'),\n",
       "  ('ie', 'ie'),\n",
       "  ('s', 's')],\n",
       " [('th', 'th'), ('at', 'at')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('n', 'no')],\n",
       " [('e', 'ne'), ('ed', 'ed')],\n",
       " [('t', 'to')],\n",
       " [('x', 'ex'), ('er', 'er'), ('t', 't')],\n",
       " [('th', 'th'), ('em', 'em'), ('se', 'se'), ('lv', 'lv'), ('s', 'es')],\n",
       " [('us', 'us'), ('ua', 'ua'), ('ll', 'll'), ('y', 'y')],\n",
       " [('be', 'be'), ('co', 'co'), ('me', 'me')],\n",
       " [('bo', 'bo'), ('re', 're'), ('d', 'd')],\n",
       " [('he', 'he'), ('do', 'do'), ('ni', 'ni'), ('s', 'st'), ('ic', 'ic')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('de', 'de'),\n",
       "  ('mo', 'mo'),\n",
       "  ('ra', 'ra'),\n",
       "  ('li', 'li'),\n",
       "  ('ze', 'ze'),\n",
       "  ('d', 'd')],\n",
       " [('ev', 'ev'), ('en', 'en')],\n",
       " [('th', 'th'), ('ou', 'ou'), ('gh', 'gh')],\n",
       " [('th', 'th'), ('ey', 'ey')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('po', 'po'), ('we', 'we'), ('r', 'r')],\n",
       " [('th', 'th'), ('is', 'is')],\n",
       " [('sh', 'sh'), ('ow', 'ow'), ('s', 's')],\n",
       " [('th', 'th'), ('at', 'at')],\n",
       " [('po', 'po'), ('we', 'we'), ('r', 'r')],\n",
       " [('is', 'is')],\n",
       " [('no', 'no'), ('t', 't')],\n",
       " [('en', 'en'), ('ou', 'ou'), ('gh', 'gh')],\n",
       " [('on', 'on'), ('e', 'e')],\n",
       " [('mu', 'mu'), ('st', 'st')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('go', 'go'), ('al', 'al'), ('s', 's')],\n",
       " [('to', 'to'), ('wa', 'wa'), ('rd', 'rd')],\n",
       " [('wh', 'wh'), ('ic', 'ic'), ('h', 'h')],\n",
       " [('to', 'to')],\n",
       " [('ex', 'ex'), ('er', 'er'), ('ci', 'ci'), ('se', 'se')],\n",
       " [('on', 'on'), ('e', 'e')],\n",
       " [('s', 's')],\n",
       " [('po', 'po'), ('we', 'we'), ('r', 'r')],\n",
       " [('ev', 'ev'), ('r', 'er'), ('yo', 'yo'), ('ne', 'ne')],\n",
       " [('ha', 'ha'), ('s', 's')],\n",
       " [('go', 'go'), ('al', 'al'), ('s', 's')],\n",
       " [('i', 'if')],\n",
       " [('no', 'no'), ('th', 'th'), ('in', 'in'), ('g', 'g')],\n",
       " [('el', 'el'), ('se', 'se')],\n",
       " [('to', 'to')],\n",
       " [('ob', 'ob'), ('ta', 'ta'), ('i', 'in')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('ph', 'ph'), ('ys', 'ys'), ('ic', 'ic'), ('al', 'al')],\n",
       " [('ne', 'ne'),\n",
       "  ('ce', 'ce'),\n",
       "  ('ss', 'ss'),\n",
       "  ('it', 'it'),\n",
       "  ('i', 'ie'),\n",
       "  ('s', 's')],\n",
       " [('of', 'of')],\n",
       " [('li', 'li'), ('fe', 'fe')],\n",
       " [('o', 'fo'), ('od', 'od')],\n",
       " [('wa', 'wa'), ('te', 'te'), ('r', 'r')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('wh', 'wh'), ('at', 'at'), ('ev', 'ev'), ('e', 'er')],\n",
       " [('cl', 'cl'), ('ot', 'ot'), ('hi', 'hi'), ('ng', 'ng')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('sh', 'sh'), ('el', 'el'), ('te', 'te'), ('r', 'r')],\n",
       " [('ar', 'ar'), ('e', 'e')],\n",
       " [('ma', 'ma'), ('de', 'de')],\n",
       " [('ne', 'ne'), ('ce', 'ce'), ('ss', 'ss'), ('r', 'ar'), ('y', 'y')],\n",
       " [('by', 'by')],\n",
       " [('h', 'th'), ('e', 'e')],\n",
       " [('cl', 'cl'), ('im', 'im'), ('at', 'at'), ('e', 'e')],\n",
       " [('b', 'bu'), ('t', 't')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('le', 'le'), ('is', 'is'), ('ur', 'ur'), ('ed', 'ed')],\n",
       " [('ar', 'ar'), ('is', 'is'), ('to', 'to'), ('cr', 'cr'), ('at', 'at')],\n",
       " [('ob', 'ob'), ('a', 'ta'), ('in', 'in'), ('s', 's')],\n",
       " [('th', 'th'), ('es', 'es'), ('e', 'e')],\n",
       " [('th', 'th'), ('in', 'in'), ('gs', 'gs')],\n",
       " [('wi', 'wi'), ('th', 'th'), ('ou', 'ou'), ('t', 't')],\n",
       " [('ef', 'ef'), ('fo', 'fo'), ('rt', 'rt')],\n",
       " [('he', 'he'), ('nc', 'nc'), ('e', 'e')],\n",
       " [('hi', 'hi'), ('s', 's')],\n",
       " [('bo', 'bo'), ('re', 're'), ('do', 'do'), ('m', 'm')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('de', 'de'),\n",
       "  ('mo', 'mo'),\n",
       "  ('ra', 'ra'),\n",
       "  ('li', 'li'),\n",
       "  ('za', 'za'),\n",
       "  ('ti', 'ti'),\n",
       "  ('on', 'on')],\n",
       " [('no', 'no'),\n",
       "  ('na', 'na'),\n",
       "  ('tt', 'tt'),\n",
       "  ('ai', 'ai'),\n",
       "  ('nm', 'nm'),\n",
       "  ('en', 'en'),\n",
       "  ('t', 't')],\n",
       " [('o', 'of')],\n",
       " [('im', 'im'), ('po', 'po'), ('t', 'rt'), ('n', 'an'), ('t', 't')],\n",
       " [('go', 'go'), ('al', 'al'), ('s', 's')],\n",
       " [('re', 're'), ('su', 'su'), ('lt', 'lt'), ('s', 's')],\n",
       " [('in', 'in')],\n",
       " [('de', 'de'), ('at', 'at'), ('h', 'h')],\n",
       " [('if', 'if')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('go', 'go'), ('al', 'al'), ('s', 's')],\n",
       " [('ar', 'ar'), ('e', 'e')],\n",
       " [('ph', 'ph'), ('ys', 'ys'), ('ic', 'ic'), ('al', 'al')],\n",
       " [('ne', 'ne'),\n",
       "  ('ce', 'ce'),\n",
       "  ('ss', 'ss'),\n",
       "  ('it', 'it'),\n",
       "  ('ie', 'ie'),\n",
       "  ('s', 's')],\n",
       " [('a', 'an'), ('d', 'd')],\n",
       " [('in', 'in')],\n",
       " [('fr', 'fr'),\n",
       "  ('us', 'us'),\n",
       "  ('tr', 'tr'),\n",
       "  ('at', 'at'),\n",
       "  ('io', 'io'),\n",
       "  ('n', 'n')],\n",
       " [('if', 'if')],\n",
       " [('no', 'no'),\n",
       "  ('na', 'na'),\n",
       "  ('tt', 'tt'),\n",
       "  ('ai', 'ai'),\n",
       "  ('nm', 'nm'),\n",
       "  ('en', 'en'),\n",
       "  ('t', 't')],\n",
       " [('of', 'of')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('go', 'go'), ('al', 'al'), ('s', 's')],\n",
       " [('i', 'is')],\n",
       " [('co', 'co'), ('p', 'mp'), ('at', 'at'), ('ib', 'ib'), ('le', 'le')],\n",
       " [('wi', 'wi'), ('th', 'th')],\n",
       " [('su', 'su'), ('rv', 'rv'), ('iv', 'iv'), ('al', 'al')],\n",
       " [('co', 'co'), ('ns', 'ns'), ('is', 'is'), ('t', 'te'), ('nt', 'nt')],\n",
       " [('f', 'fa'), ('il', 'il'), ('ur', 'ur'), ('e', 'e')],\n",
       " [('to', 'to')],\n",
       " [('at', 'at'), ('ta', 'ta'), ('in', 'in')],\n",
       " [('go', 'go'), ('al', 'al'), ('s', 's')],\n",
       " [('th', 'th'), ('ro', 'ro'), ('ug', 'ug'), ('ho', 'ho'), ('ut', 'ut')],\n",
       " [('li', 'li'), ('fe', 'fe')],\n",
       " [('re', 're'), ('su', 'su'), ('lt', 'lt'), ('s', 's')],\n",
       " [('in', 'in')],\n",
       " [('de', 'de'), ('fe', 'fe'), ('at', 'at'), ('is', 'is'), ('m', 'm')],\n",
       " [('lo', 'lo'), ('w', 'w')],\n",
       " [('se', 'se'), ('lf', 'lf')],\n",
       " [('es', 'es'), ('e', 'te'), ('em', 'em')],\n",
       " [('or', 'or')],\n",
       " [('de', 'de'), ('pr', 'pr'), ('s', 'es'), ('si', 'si'), ('on', 'on')],\n",
       " [('th', 'th'), ('us', 'us')],\n",
       " [('i', 'in')],\n",
       " [('or', 'or'), ('de', 'de'), ('r', 'r')],\n",
       " [('to', 'to')],\n",
       " [('av', 'av'), ('oi', 'oi'), ('d', 'd')],\n",
       " [('se', 'se'), ('ri', 'ri'), ('ou', 'ou'), ('s', 's')],\n",
       " [('ps', 'ps'),\n",
       "  ('yc', 'yc'),\n",
       "  ('ho', 'ho'),\n",
       "  ('lo', 'lo'),\n",
       "  ('gi', 'gi'),\n",
       "  ('ca', 'ca'),\n",
       "  ('l', 'l')],\n",
       " [('pr', 'pr'), ('b', 'ob'), ('le', 'le'), ('ms', 'ms')],\n",
       " [('a', 'a')],\n",
       " [('hu', 'hu'), ('ma', 'ma'), ('n', 'n')],\n",
       " [('be', 'be'), ('in', 'in'), ('g', 'g')],\n",
       " [('ne', 'ne'), ('ed', 'ed'), ('s', 's')],\n",
       " [('go', 'go'), ('al', 'al'), ('s', 's')],\n",
       " [('wh', 'wh'), ('os', 'os'), ('e', 'e')],\n",
       " [('at', 'at'), ('ta', 'ta'), ('in', 'in'), ('me', 'me'), ('nt', 'nt')],\n",
       " [('re', 're'), ('qu', 'qu'), ('ir', 'ir'), ('es', 'es')],\n",
       " [('ef', 'ef'), ('fo', 'fo'), ('rt', 'rt')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('he', 'he')],\n",
       " [('mu', 'mu'), ('st', 'st')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('a', 'a')],\n",
       " [('re', 're'), ('a', 'as'), ('on', 'on'), ('ab', 'ab'), ('le', 'le')],\n",
       " [('ra', 'ra'), ('te', 'te')],\n",
       " [('of', 'of')],\n",
       " [('su', 'su'), ('cc', 'cc'), ('s', 'es'), ('s', 's')],\n",
       " [('n', 'in')],\n",
       " [('at', 'at'), ('ta', 'ta'), ('in', 'in'), ('in', 'in'), ('g', 'g')],\n",
       " [('hi', 'hi'), ('s', 's')],\n",
       " [('go', 'go'), ('al', 'al'), ('s', 's')],\n",
       " [('su', 'su'), ('rr', 'rr'), ('og', 'og'), ('at', 'at'), ('e', 'e')],\n",
       " [('ac', 'ac'), ('ti', 'ti'), ('vi', 'vi'), ('ti', 'ti'), ('es', 'es')],\n",
       " [('bu', 'bu'), ('t', 't')],\n",
       " [('no', 'no'), ('t', 't')],\n",
       " [('ev', 'ev'), ('er', 'er'), ('y', 'y')],\n",
       " [('le', 'le'), ('is', 'is'), ('ur', 'ur'), ('ed', 'ed')],\n",
       " [('ar', 'ar'), ('is', 'is'), ('to', 'to'), ('cr', 'cr'), ('at', 'at')],\n",
       " [('be', 'be'), ('co', 'co'), ('me', 'me'), ('s', 's')],\n",
       " [('bo', 'bo'), ('re', 're'), ('d', 'd')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('de', 'de'),\n",
       "  ('mo', 'mo'),\n",
       "  ('ra', 'ra'),\n",
       "  ('li', 'li'),\n",
       "  ('ze', 'ze'),\n",
       "  ('d', 'd')],\n",
       " [('fo', 'fo'), ('r', 'r')],\n",
       " [('ex', 'ex'), ('am', 'am'), ('pl', 'pl'), ('e', 'e')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('e', 'em'), ('pe', 'pe'), ('ro', 'ro'), ('r', 'r')],\n",
       " [('hi', 'hi'), ('ro', 'ro'), ('hi', 'hi'), ('o', 'to')],\n",
       " [('in', 'in'), ('st', 'st'), ('ea', 'ea'), ('d', 'd')],\n",
       " [('of', 'of')],\n",
       " [('si', 'si'), ('nk', 'nk'), ('in', 'in'), ('g', 'g')],\n",
       " [('in', 'in'), ('to', 'to')],\n",
       " [('d', 'de'), ('ca', 'ca'), ('de', 'de'), ('nt', 'nt')],\n",
       " [('he', 'he'), ('do', 'do'), ('ni', 'ni'), ('sm', 'sm')],\n",
       " [('de', 'de'), ('vo', 'vo'), ('te', 'te'), ('d', 'd')],\n",
       " [('hi', 'hi'), ('ms', 'ms'), ('el', 'el'), ('f', 'f')],\n",
       " [('to', 'to')],\n",
       " [('ma', 'ma'), ('ri', 'ri'), ('ne', 'ne')],\n",
       " [('bi', 'bi'), ('ol', 'ol'), ('og', 'og'), ('y', 'y')],\n",
       " [('a', 'a')],\n",
       " [('fi', 'fi'), ('el', 'el'), ('d', 'd')],\n",
       " [('in', 'in')],\n",
       " [('wh', 'wh'), ('ic', 'ic'), ('h', 'h')],\n",
       " [('he', 'he')],\n",
       " [('be', 'be'), ('ca', 'ca'), ('me', 'me')],\n",
       " [('di', 'di'),\n",
       "  ('st', 'st'),\n",
       "  ('in', 'in'),\n",
       "  ('gu', 'gu'),\n",
       "  ('is', 'is'),\n",
       "  ('he', 'he'),\n",
       "  ('d', 'd')],\n",
       " [('wh', 'wh'), ('en', 'en')],\n",
       " [('pe', 'pe'), ('op', 'op'), ('le', 'le')],\n",
       " [('o', 'do')],\n",
       " [('n', 'no'), ('t', 't')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('t', 'to')],\n",
       " [('ex', 'ex'), ('er', 'er'), ('t', 't')],\n",
       " [('th', 'th'), ('em', 'em'), ('se', 'se'), ('lv', 'lv'), ('es', 'es')],\n",
       " [('to', 'to')],\n",
       " [('sa', 'sa'), ('ti', 'ti'), ('sf', 'sf'), ('y', 'y')],\n",
       " [('th', 'th'), ('ei', 'ei'), ('r', 'r')],\n",
       " [('ph', 'ph'), ('ys', 'ys'), ('ic', 'ic'), ('al', 'al')],\n",
       " [('ne', 'ne'), ('ed', 'ed'), ('s', 's')],\n",
       " [('th', 'th'), ('ey', 'ey')],\n",
       " [('of', 'of'), ('te', 'te'), ('n', 'n')],\n",
       " [('e', 'se'), ('t', 't')],\n",
       " [('up', 'up')],\n",
       " [('ar', 'ar'), ('ti', 'ti'), ('fi', 'fi'), ('ci', 'ci'), ('al', 'al')],\n",
       " [('go', 'go'), ('al', 'al'), ('s', 's')],\n",
       " [('fo', 'fo'), ('r', 'r')],\n",
       " [('th', 'th'), ('em', 'em'), ('se', 'se'), ('lv', 'lv'), ('es', 'es')],\n",
       " [('in', 'in')],\n",
       " [('ma', 'ma'), ('ny', 'ny')],\n",
       " [('ca', 'ca'), ('se', 'se'), ('s', 's')],\n",
       " [('th', 'th'), ('ey', 'ey')],\n",
       " [('th', 'th'), ('en', 'en')],\n",
       " [('pu', 'pu'), ('rs', 'rs'), ('ue', 'ue')],\n",
       " [('th', 'th'), ('es', 'es'), ('e', 'e')],\n",
       " [('go', 'go'), ('al', 'al'), ('s', 's')],\n",
       " [('wi', 'wi'), ('th', 'th')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('sa', 'sa'), ('me', 'me')],\n",
       " [('en', 'en'), ('er', 'er'), ('gy', 'gy')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('em', 'em'), ('ot', 'ot'), ('io', 'io'), ('na', 'na'), ('l', 'l')],\n",
       " [('in', 'in'),\n",
       "  ('vo', 'vo'),\n",
       "  ('lv', 'lv'),\n",
       "  ('em', 'em'),\n",
       "  ('en', 'en'),\n",
       "  ('t', 't')],\n",
       " [('th', 'th'), ('at', 'at')],\n",
       " [('th', 'th'), ('ey', 'ey')],\n",
       " [('ot', 'ot'), ('he', 'he'), ('rw', 'rw'), ('is', 'is'), ('e', 'e')],\n",
       " [('wo', 'wo'), ('ul', 'ul'), ('d', 'd')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('pu', 'pu'), ('t', 't')],\n",
       " [('i', 'in'), ('to', 'to')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('s', 'se'), ('ar', 'ar'), ('ch', 'ch')],\n",
       " [('fo', 'fo'), ('r', 'r')],\n",
       " [('ph', 'ph'), ('ys', 'ys'), ('ic', 'ic'), ('a', 'al')],\n",
       " [('ne', 'ne'),\n",
       "  ('ce', 'ce'),\n",
       "  ('ss', 'ss'),\n",
       "  ('it', 'it'),\n",
       "  ('ie', 'ie'),\n",
       "  ('s', 's')],\n",
       " [('th', 'th'), ('us', 'us')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('ar', 'ar'),\n",
       "  ('is', 'is'),\n",
       "  ('t', 'to'),\n",
       "  ('cr', 'cr'),\n",
       "  ('at', 'at'),\n",
       "  ('s', 's')],\n",
       " [('of', 'of')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('ro', 'ro'), ('ma', 'ma'), ('n', 'n')],\n",
       " [('em', 'em'), ('pi', 'pi'), ('re', 're')],\n",
       " [('ha', 'ha'), ('d', 'd')],\n",
       " [('th', 'th'), ('ei', 'ei'), ('r', 'r')],\n",
       " [('li', 'li'), ('te', 'te'), ('ra', 'ra'), ('ry', 'ry')],\n",
       " [('pr', 'pr'),\n",
       "  ('et', 'et'),\n",
       "  ('en', 'en'),\n",
       "  ('ti', 'ti'),\n",
       "  ('on', 'on'),\n",
       "  ('s', 's')],\n",
       " [('ma', 'ma'), ('ny', 'ny')],\n",
       " [('u', 'eu'), ('ro', 'ro'), ('pe', 'pe'), ('an', 'an')],\n",
       " [('ar', 'ar'),\n",
       "  ('is', 'is'),\n",
       "  ('to', 'to'),\n",
       "  ('cr', 'cr'),\n",
       "  ('at', 'at'),\n",
       "  ('s', 's')],\n",
       " [('a', 'a')],\n",
       " [('fe', 'fe'), ('w', 'w')],\n",
       " [('ce', 'ce'), ('nt', 'nt'), ('ur', 'ur'), ('ie', 'ie'), ('s', 's')],\n",
       " [('ag', 'ag'), ('o', 'o')],\n",
       " [('in', 'in'), ('ve', 've'), ('st', 'st'), ('ed', 'ed')],\n",
       " [('tr', 'tr'), ('m', 'em'), ('en', 'en'), ('do', 'do'), ('us', 'us')],\n",
       " [('ti', 'ti'), ('me', 'me')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('en', 'en'), ('er', 'er'), ('gy', 'gy')],\n",
       " [('in', 'in')],\n",
       " [('hu', 'hu'), ('nt', 'nt'), ('in', 'in'), ('g', 'g')],\n",
       " [('th', 'th'), ('ou', 'ou'), ('gh', 'gh')],\n",
       " [('t', 'th'), ('ey', 'ey')],\n",
       " [('ce', 'ce'), ('rt', 'rt'), ('ai', 'ai'), ('nl', 'nl'), ('y', 'y')],\n",
       " [('i', 'di'), ('dn', 'dn')],\n",
       " [('t', 't')],\n",
       " [('ne', 'ne'), ('ed', 'ed')],\n",
       " [('t', 'th'), ('e', 'e')],\n",
       " [('me', 'me'), ('at', 'at')],\n",
       " [('ot', 'ot'), ('he', 'he'), ('r', 'r')],\n",
       " [('ar', 'ar'),\n",
       "  ('is', 'is'),\n",
       "  ('to', 'to'),\n",
       "  ('cr', 'cr'),\n",
       "  ('ac', 'ac'),\n",
       "  ('ie', 'ie'),\n",
       "  ('s', 's')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('co', 'co'), ('mp', 'mp'), ('t', 'et'), ('ed', 'ed')],\n",
       " [('fo', 'fo'), ('r', 'r')],\n",
       " [('st', 'st'), ('at', 'at'), ('us', 'us')],\n",
       " [('th', 'th'), ('ro', 'ro'), ('ug', 'ug'), ('h', 'h')],\n",
       " [('el', 'el'), ('ab', 'ab'), ('or', 'or'), ('at', 'at'), ('e', 'e')],\n",
       " [('di', 'di'), ('sp', 'sp'), ('la', 'la'), ('ys', 'ys')],\n",
       " [('of', 'of')],\n",
       " [('we', 'we'), ('al', 'al'), ('th', 'th')],\n",
       " [('n', 'an'), ('d', 'd')],\n",
       " [('a', 'a')],\n",
       " [('fe', 'fe'), ('w', 'w')],\n",
       " [('ar', 'ar'),\n",
       "  ('is', 'is'),\n",
       "  ('to', 'to'),\n",
       "  ('cr', 'cr'),\n",
       "  ('at', 'at'),\n",
       "  ('s', 's')],\n",
       " [('li', 'li'), ('ke', 'ke')],\n",
       " [('hi', 'hi'), ('ro', 'ro'), ('hi', 'hi'), ('to', 'to')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('tu', 'tu'), ('n', 'rn'), ('ed', 'ed')],\n",
       " [('to', 'to')],\n",
       " [('sc', 'sc'), ('ie', 'ie'), ('nc', 'nc'), ('e', 'e')],\n",
       " [('we', 'we')],\n",
       " [('us', 'us'), ('e', 'e')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('te', 'te'), ('m', 'rm')],\n",
       " [('su', 'su'), ('rr', 'rr'), ('og', 'og'), ('at', 'at'), ('e', 'e')],\n",
       " [('ac', 'ac'), ('ti', 'ti'), ('vi', 'vi'), ('ty', 'ty')],\n",
       " [('to', 'to')],\n",
       " [('de', 'de'), ('si', 'si'), ('gn', 'gn'), ('at', 'at'), ('e', 'e')],\n",
       " [('an', 'an')],\n",
       " [('ac', 'ac'), ('ti', 'ti'), ('vi', 'vi'), ('ty', 'ty')],\n",
       " [('th', 'th'), ('at', 'at')],\n",
       " [('is', 'is')],\n",
       " [('di', 'di'), ('re', 're'), ('ct', 'ct'), ('ed', 'ed')],\n",
       " [('to', 'to'), ('wa', 'wa'), ('rd', 'rd')],\n",
       " [('an', 'an')],\n",
       " [('ar', 'ar'), ('ti', 'ti'), ('fi', 'fi'), ('ci', 'ci'), ('al', 'al')],\n",
       " [('go', 'go'), ('al', 'al')],\n",
       " [('th', 'th'), ('at', 'at')],\n",
       " [('pe', 'pe'), ('p', 'op'), ('le', 'le')],\n",
       " [('se', 'se'), ('t', 't')],\n",
       " [('up', 'up')],\n",
       " [('fo', 'fo'), ('r', 'r')],\n",
       " [('th', 'th'), ('em', 'em'), ('se', 'se'), ('v', 'lv'), ('es', 'es')],\n",
       " [('me', 'me'), ('re', 're'), ('ly', 'ly')],\n",
       " [('in', 'in')],\n",
       " [('or', 'or'), ('de', 'de'), ('r', 'r')],\n",
       " [('to', 'to')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('so', 'so'), ('me', 'me')],\n",
       " [('go', 'go'), ('al', 'al')],\n",
       " [('o', 'to')],\n",
       " [('wo', 'wo'), ('rk', 'rk')],\n",
       " [('to', 'to'), ('w', 'wa'), ('rd', 'rd')],\n",
       " [('or', 'or')],\n",
       " [('le', 'le'), ('t', 't')],\n",
       " [('us', 'us')],\n",
       " [('sa', 'sa'), ('y', 'y')],\n",
       " [('me', 'me'), ('re', 're'), ('ly', 'ly')],\n",
       " [('fo', 'fo'), ('r', 'r')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('s', 'sa'), ('ke', 'ke')],\n",
       " [('f', 'of')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('fu', 'fu'),\n",
       "  ('lf', 'lf'),\n",
       "  ('il', 'il'),\n",
       "  ('lm', 'lm'),\n",
       "  ('en', 'en'),\n",
       "  ('t', 't')],\n",
       " [('h', 'th'), ('at', 'at')],\n",
       " [('th', 'th'), ('ey', 'ey')],\n",
       " [('ge', 'ge'), ('t', 't')],\n",
       " [('fr', 'fr'), ('om', 'om')],\n",
       " [('pu', 'pu'), ('rs', 'rs'), ('ui', 'ui'), ('ng', 'ng')],\n",
       " [('h', 'th'), ('e', 'e')],\n",
       " [('go', 'go'), ('al', 'al')],\n",
       " [('he', 'he'), ('re', 're')],\n",
       " [('is', 'is')],\n",
       " [('a', 'a')],\n",
       " [('ru', 'ru'), ('le', 'le')],\n",
       " [('of', 'of')],\n",
       " [('th', 'th'), ('um', 'um'), ('b', 'b')],\n",
       " [('fo', 'fo'), ('r', 'r')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('id', 'id'),\n",
       "  ('en', 'en'),\n",
       "  ('ti', 'ti'),\n",
       "  ('fi', 'fi'),\n",
       "  ('ca', 'ca'),\n",
       "  ('ti', 'ti'),\n",
       "  ('on', 'on')],\n",
       " [('of', 'of')],\n",
       " [('su', 'su'), ('rr', 'rr'), ('og', 'og'), ('at', 'at'), ('e', 'e')],\n",
       " [('ac', 'ac'), ('ti', 'ti'), ('vi', 'vi'), ('ti', 'ti'), ('es', 'es')],\n",
       " [('gi', 'gi'), ('ve', 've'), ('n', 'n')],\n",
       " [('a', 'a')],\n",
       " [('e', 'pe'), ('rs', 'rs'), ('on', 'on')],\n",
       " [('wh', 'wh'), ('o', 'o')],\n",
       " [('de', 'de'), ('vo', 'vo'), ('te', 'te'), ('s', 's')],\n",
       " [('mu', 'mu'), ('ch', 'ch')],\n",
       " [('ti', 'ti'), ('me', 'me')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('en', 'en'), ('er', 'er'), ('gy', 'gy')],\n",
       " [('to', 'to')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('pu', 'pu'), ('rs', 'rs'), ('ui', 'ui'), ('t', 't')],\n",
       " [('of', 'of')],\n",
       " [('go', 'go'), ('l', 'al')],\n",
       " [('x', 'x')],\n",
       " [('as', 'as'), ('k', 'k')],\n",
       " [('yo', 'yo'), ('ur', 'ur'), ('se', 'se'), ('lf', 'lf')],\n",
       " [('h', 'th'), ('is', 'is')],\n",
       " [('if', 'if')],\n",
       " [('he', 'he')],\n",
       " [('a', 'ha'), ('d', 'd')],\n",
       " [('to', 'to')],\n",
       " [('de', 'de'), ('vo', 'vo'), ('te', 'te')],\n",
       " [('mo', 'mo'), ('st', 'st')],\n",
       " [('of', 'of')],\n",
       " [('hi', 'hi'), ('s', 's')],\n",
       " [('ti', 'ti'), ('me', 'me')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('en', 'en'), ('e', 'er'), ('gy', 'gy')],\n",
       " [('to', 'to')],\n",
       " [('sa', 'sa'), ('ti', 'ti'), ('sf', 'sf'), ('yi', 'yi'), ('ng', 'ng')],\n",
       " [('hi', 'hi'), ('s', 's')],\n",
       " [('bi', 'bi'), ('ol', 'ol'), ('og', 'og'), ('ic', 'ic'), ('al', 'al')],\n",
       " [('ne', 'ne'), ('ed', 'ed'), ('s', 's')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('f', 'if')],\n",
       " [('t', 'th'), ('a', 'at')],\n",
       " [('ef', 'ef'), ('fo', 'fo'), ('r', 'rt')],\n",
       " [('re', 're'), ('qu', 'qu'), ('ir', 'ir'), ('ed', 'ed')],\n",
       " [('hi', 'hi'), ('m', 'm')],\n",
       " [('o', 'to')],\n",
       " [('us', 'us'), ('e', 'e')],\n",
       " [('hi', 'hi'), ('s', 's')],\n",
       " [('ph', 'ph'), ('ys', 'ys'), ('ic', 'ic'), ('al', 'al')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('me', 'me'), ('nt', 'nt'), ('al', 'al')],\n",
       " [('fa', 'fa'), ('ci', 'ci'), ('li', 'li'), ('ti', 'ti'), ('es', 'es')],\n",
       " [('in', 'in')],\n",
       " [('a', 'a')],\n",
       " [('va', 'va'), ('ri', 'ri'), ('ed', 'ed')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('in', 'in'),\n",
       "  ('te', 'te'),\n",
       "  ('re', 're'),\n",
       "  ('st', 'st'),\n",
       "  ('in', 'in'),\n",
       "  ('g', 'g')],\n",
       " [('wa', 'wa'), ('y', 'y')],\n",
       " [('w', 'wo'), ('ul', 'ul'), ('d', 'd')],\n",
       " [('he', 'he')],\n",
       " [('fe', 'fe'), ('el', 'el')],\n",
       " [('se', 'se'), ('ri', 'ri'), ('ou', 'ou'), ('sl', 'sl'), ('y', 'y')],\n",
       " [('de', 'de'), ('pr', 'pr'), ('iv', 'iv'), ('ed', 'ed')],\n",
       " [('be', 'be'), ('ca', 'ca'), ('us', 'us'), ('e', 'e')],\n",
       " [('he', 'he')],\n",
       " [('di', 'di'), ('d', 'd')],\n",
       " [('no', 'no'), ('t', 't')],\n",
       " [('t', 'at'), ('ta', 'ta'), ('in', 'in')],\n",
       " [('go', 'go'), ('al', 'al')],\n",
       " [('x', 'x')],\n",
       " [('if', 'if')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('an', 'an'), ('sw', 'sw'), ('er', 'er')],\n",
       " [('is', 'is')],\n",
       " [('o', 'no')],\n",
       " [('th', 'th'), ('en', 'en')],\n",
       " [('h', 'th'), ('e', 'e')],\n",
       " [('pe', 'pe'), ('rs', 'rs'), ('on', 'on')],\n",
       " [('s', 's')],\n",
       " [('pu', 'pu'), ('rs', 'rs'), ('ui', 'ui'), ('t', 't')],\n",
       " [('of', 'of')],\n",
       " [('a', 'a')],\n",
       " [('go', 'go'), ('al', 'al')],\n",
       " [('x', 'x')],\n",
       " [('is', 'is')],\n",
       " [('a', 'a')],\n",
       " [('su', 'su'), ('rr', 'rr'), ('og', 'og'), ('at', 'at'), ('e', 'e')],\n",
       " [('ac', 'ac'), ('ti', 'ti'), ('vi', 'vi'), ('ty', 'ty')],\n",
       " [('hi', 'hi'), ('ro', 'ro'), ('hi', 'hi'), ('to', 'to')],\n",
       " [('s', 's')],\n",
       " [('st', 'st'), ('ud', 'ud'), ('ie', 'ie'), ('s', 's')],\n",
       " [('in', 'in')],\n",
       " [('ma', 'ma'), ('ri', 'ri'), ('ne', 'ne')],\n",
       " [('bi', 'bi'), ('ol', 'ol'), ('og', 'og'), ('y', 'y')],\n",
       " [('cl', 'cl'), ('ea', 'ea'), ('rl', 'rl'), ('y', 'y')],\n",
       " [('co', 'co'),\n",
       "  ('ns', 'ns'),\n",
       "  ('ti', 'ti'),\n",
       "  ('tu', 'tu'),\n",
       "  ('te', 'te'),\n",
       "  ('d', 'd')],\n",
       " [('a', 'a')],\n",
       " [('su', 'su'), ('rr', 'rr'), ('og', 'og'), ('at', 'at'), ('e', 'e')],\n",
       " [('ac', 'ac'), ('ti', 'ti'), ('i', 'vi'), ('ty', 'ty')],\n",
       " [('si', 'si'), ('nc', 'nc'), ('e', 'e')],\n",
       " [('it', 'it')],\n",
       " [('s', 'is')],\n",
       " [('pr', 'pr'), ('et', 'et'), ('ty', 'ty')],\n",
       " [('ce', 'ce'), ('rt', 'rt'), ('ai', 'ai'), ('n', 'n')],\n",
       " [('th', 'th'), ('at', 'at')],\n",
       " [('if', 'if')],\n",
       " [('i', 'hi'), ('ro', 'ro'), ('hi', 'hi'), ('to', 'to')],\n",
       " [('ha', 'ha'), ('d', 'd')],\n",
       " [('h', 'ha'), ('d', 'd')],\n",
       " [('to', 'to')],\n",
       " [('sp', 'sp'), ('en', 'en'), ('d', 'd')],\n",
       " [('hi', 'hi'), ('s', 's')],\n",
       " [('t', 'ti'), ('me', 'me')],\n",
       " [('wo', 'wo'), ('k', 'rk'), ('in', 'in'), ('g', 'g')],\n",
       " [('at', 'at')],\n",
       " [('in', 'in'),\n",
       "  ('te', 'te'),\n",
       "  ('re', 're'),\n",
       "  ('st', 'st'),\n",
       "  ('in', 'in'),\n",
       "  ('g', 'g')],\n",
       " [('no', 'no'), ('n', 'n')],\n",
       " [('sc', 'sc'), ('ie', 'ie'), ('nt', 'nt'), ('if', 'if'), ('ic', 'ic')],\n",
       " [('ta', 'ta'), ('sk', 'sk'), ('s', 's')],\n",
       " [('in', 'in')],\n",
       " [('or', 'or'), ('de', 'de'), ('r', 'r')],\n",
       " [('to', 'to')],\n",
       " [('ob', 'ob'), ('ta', 'ta'), ('in', 'in')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('ne', 'ne'),\n",
       "  ('ce', 'ce'),\n",
       "  ('ss', 'ss'),\n",
       "  ('it', 'it'),\n",
       "  ('ie', 'ie'),\n",
       "  ('s', 's')],\n",
       " [('of', 'of')],\n",
       " [('li', 'li'), ('fe', 'fe')],\n",
       " [('e', 'he')],\n",
       " [('wo', 'wo'), ('ul', 'ul'), ('d', 'd')],\n",
       " [('no', 'no'), ('t', 't')],\n",
       " [('ha', 'ha'), ('ve', 've')],\n",
       " [('fe', 'fe'), ('t', 'lt')],\n",
       " [('de', 'de'), ('pr', 'pr'), ('iv', 'iv'), ('ed', 'ed')],\n",
       " [('be', 'be'), ('ca', 'ca'), ('us', 'us'), ('e', 'e')],\n",
       " [('he', 'he')],\n",
       " [('di', 'di'), ('d', 'dn')],\n",
       " [('t', 't')],\n",
       " [('kn', 'kn'), ('o', 'ow')],\n",
       " [('al', 'al'), ('l', 'l')],\n",
       " [('ab', 'ab'), ('ou', 'ou'), ('t', 't')],\n",
       " [('t', 'th'), ('e', 'e')],\n",
       " [('an', 'an'), ('at', 'at'), ('m', 'om'), ('y', 'y')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('li', 'li'), ('fe', 'fe')],\n",
       " [('cy', 'cy'), ('cl', 'cl'), ('es', 'es')],\n",
       " [('f', 'of')],\n",
       " [('ma', 'ma'), ('ri', 'ri'), ('ne', 'ne')],\n",
       " [('an', 'an'), ('im', 'im'), ('al', 'al'), ('s', 's')],\n",
       " [('o', 'on')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('ot', 'ot'), ('he', 'he'), ('r', 'r')],\n",
       " [('ha', 'ha'), ('nd', 'nd')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('pu', 'pu'), ('rs', 'rs'), ('ui', 'ui'), ('t', 't')],\n",
       " [('of', 'of')],\n",
       " [('se', 'se'), ('x', 'x')],\n",
       " [('an', 'an'), ('d', 'd')],\n",
       " [('o', 'lo'), ('ve', 've')],\n",
       " [('fo', 'fo'), ('r', 'r')],\n",
       " [('ex', 'ex'), ('am', 'am'), ('l', 'pl'), ('e', 'e')],\n",
       " [('is', 'is')],\n",
       " [('no', 'no'), ('t', 't')],\n",
       " [('a', 'a')],\n",
       " [('su', 'su'), ('rr', 'rr'), ('og', 'og'), ('at', 'at'), ('e', 'e')],\n",
       " [('ac', 'ac'), ('ti', 'ti'), ('vi', 'vi'), ('ty', 'ty')],\n",
       " [('e', 'be'), ('ca', 'ca'), ('us', 'us'), ('e', 'e')],\n",
       " [('m', 'mo'), ('st', 'st')],\n",
       " [('pe', 'pe'), ('op', 'op'), ('le', 'le')],\n",
       " [('ev', 'ev'), ('en', 'en')],\n",
       " [('if', 'if')],\n",
       " [('th', 'th'), ('ei', 'ei'), ('r', 'r')],\n",
       " [('ex', 'ex'), ('is', 'is'), ('te', 'te'), ('nc', 'nc'), ('e', 'e')],\n",
       " [('we', 'we'), ('r', 're')],\n",
       " [('ot', 'ot'), ('he', 'he'), ('rw', 'rw'), ('is', 'is'), ('e', 'e')],\n",
       " [('sa', 'sa'),\n",
       "  ('ti', 'ti'),\n",
       "  ('s', 'sf'),\n",
       "  ('ac', 'ac'),\n",
       "  ('to', 'to'),\n",
       "  ('ry', 'ry')],\n",
       " [('o', 'wo'), ('ul', 'ul'), ('d', 'd')],\n",
       " [('fe', 'fe'), ('el', 'el')],\n",
       " [('de', 'de'), ('pr', 'pr'), ('iv', 'iv'), ('ed', 'ed')],\n",
       " [('if', 'if')],\n",
       " [('th', 'th'), ('ey', 'ey')],\n",
       " [('pa', 'pa'), ('ss', 'ss'), ('ed', 'ed')],\n",
       " [('h', 'th'), ('ei', 'ei'), ('r', 'r')],\n",
       " [('li', 'li'), ('ve', 've'), ('s', 's')],\n",
       " [('wi', 'wi'), ('t', 'th'), ('ou', 'ou'), ('t', 't')],\n",
       " [('ev', 'ev'), ('er', 'er')],\n",
       " [('ha', 'ha'), ('vi', 'vi'), ('ng', 'ng')],\n",
       " [('a', 'a')],\n",
       " [('re', 're'),\n",
       "  ('la', 'la'),\n",
       "  ('ti', 'ti'),\n",
       "  ('on', 'on'),\n",
       "  ('sh', 'sh'),\n",
       "  ('ip', 'ip')],\n",
       " [('wi', 'wi'), ('th', 'th')],\n",
       " [('a', 'a')],\n",
       " [('me', 'me'), ('mb', 'mb'), ('er', 'er')],\n",
       " [('of', 'of')],\n",
       " [('th', 'th'), ('e', 'e')],\n",
       " [('op', 'op'), ('po', 'po'), ('si', 'si'), ('te', 'te')],\n",
       " [('se', 'se'), ('x', 'x')],\n",
       " [('u', 'bu'), ('t', 't')],\n",
       " [('pu', 'pu'), ('rs', 'rs'), ('ui', 'ui'), ('t', 't')],\n",
       " [('of', 'of')],\n",
       " [('an', 'an')],\n",
       " [('ex', 'ex'), ('ce', 'ce'), ('ss', 'ss'), ('iv', 'iv'), ('e', 'e')],\n",
       " [('am', 'am'), ('ou', 'ou'), ('nt', 'nt')],\n",
       " [('of', 'of')],\n",
       " [('se', 'se'), ('x', 'x')],\n",
       " [('mo', 'mo'), ('re', 're')],\n",
       " [('t', 'th'), ('an', 'an')],\n",
       " [('n', 'on'), ('e', 'e')],\n",
       " [('re', 're'), ('al', 'al'), ('ly', 'ly')],\n",
       " [('ne', 'ne'), ('ed', 'ed'), ('s', 's')],\n",
       " [('ca', 'ca'), ('n', 'n')],\n",
       " [('be', 'be')],\n",
       " [('a', 'a')],\n",
       " [('su', 'su'), ('rr', 'rr'), ('og', 'og'), ('at', 'at'), ('e', 'e')],\n",
       " [('ac', 'ac'), ('ti', 'ti'), ('vi', 'vi'), ('ty', 'ty')],\n",
       " [('in', 'in')],\n",
       " [('o', 'mo'), ('de', 'de'), ('rn', 'rn')],\n",
       " [('in', 'in'), ('du', 'du'), ('st', 'st'), ('ri', 'ri'), ('al', 'al')],\n",
       " [('so', 'so'), ('ci', 'ci'), ('et', 'et'), ('y', 'y')],\n",
       " [('on', 'on'), ('ly', 'ly')],\n",
       " [('mi', 'mi'), ('ni', 'ni'), ('ma', 'ma'), ('l', 'l')],\n",
       " [('ef', 'ef'), ('fo', 'fo'), ('rt', 'rt')],\n",
       " [('is', 'is')],\n",
       " [('ne', 'ne'), ('ce', 'ce'), ('ss', 'ss'), ('ar', 'ar'), ('y', 'y')],\n",
       " [('t', 'to')],\n",
       " [('s', 'sa'), ('ti', 'ti'), ('sf', 'sf'), ('y', 'y')],\n",
       " [('on', 'on'), ('e', 'e')],\n",
       " [('s', 's')],\n",
       " [('ph', 'ph'), ('ys', 'ys'), ('ic', 'ic'), ('al', 'al')],\n",
       " [('ne', 'ne'), ('ed', 'ed'), ('s', 's')],\n",
       " [('t', 'it')],\n",
       " [('is', 'is')],\n",
       " [('en', 'en'), ('ou', 'ou'), ('gh', 'gh')],\n",
       " [('to', 'to')],\n",
       " [('go', 'go')],\n",
       " [('th', 'th'), ('r', 'ro'), ('ug', 'ug'), ('h', 'h')],\n",
       " [('a', 'a')],\n",
       " [('tr', 'tr'), ('ai', 'ai'), ('ni', 'ni'), ('ng', 'ng')],\n",
       " [('pr', 'pr'), ('og', 'og'), ('ra', 'ra'), ('m', 'm')],\n",
       " [('to', 'to')],\n",
       " [('ac', 'ac'), ('qu', 'qu'), ('ir', 'ir'), ('e', 'e')],\n",
       " [('so', 'so'), ('me', 'me')],\n",
       " [('pe', 'pe'), ('tt', 'tt'), ('y', 'y')],\n",
       " [('te', 'te'), ('ch', 'ch'), ('ni', 'ni'), ('ca', 'ca'), ('l', 'l')],\n",
       " [('sk', 'sk'), ('il', 'il'), ('l', 'l')],\n",
       " [('th', 'th'), ('en', 'en')],\n",
       " [('co', 'co'), ('me', 'me')],\n",
       " [('to', 'to')],\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean_train = clean_corpus(data_train_10)  \n",
    "data_clean_test = clean_corpus(data_test_10)\n",
    "\n",
    "# creer un corpus avec des paires de lettres \n",
    "# exemple : mot pair  [(f,f), ('o','o'), ('r','r'), ('m','m')] devient  [('fo', 'fo'), ('rm', 'rm')] \n",
    "# mot impair [('t','t'), ('h','h'), ('e', 'e')]   devient   [('th', 'th'), ('e', 'e')]\n",
    "def double_corpus(data):\n",
    "    double_corp =[]\n",
    "    \n",
    "    for mot in data:\n",
    "        double_mot=[]\n",
    "        #print(mot)\n",
    "        for i in range(1, len(mot), 2) :\n",
    "            \n",
    "            mot0, tag0 =mot[i-1]\n",
    "            mot1, tag1 = mot[i]\n",
    "            double_mot.append(((tag0+tag1), (tag0+ tag1)))\n",
    "        if len(mot)%2:\n",
    "            double_mot.append((mot[-1][1],mot[-1][1] ))\n",
    "        double_corp.append(double_mot)\n",
    "    return double_corp  \n",
    "\n",
    "double_corpus_train = double_corpus(data_clean_train)\n",
    "double_corpus_test = double_corpus(data_clean_test)\n",
    "\n",
    "# détruit des caractères aléatoirement dans les observation \n",
    "# ex [('vi', 'vi'), ('ol', 'ol'), ('en', 'en'), ('ce', 'ce')] devient [('i', 'vi'), ('ol', 'ol'), ('en', 'en'), ('ce', 'ce')]\n",
    "def del_carac(data, taux_erreur = 0.1):\n",
    "    import string\n",
    "    import random\n",
    "    alphabet =list(string.ascii_lowercase)\n",
    "    err = 0\n",
    "    total = 0\n",
    "    for mot in data:\n",
    "        for couple in mot:\n",
    "            total = total+1\n",
    "    taux_erreur = err / total \n",
    "    \n",
    "    while  taux_erreur <0.1:\n",
    "        i = random.randint(0,len(data)-1)\n",
    "        mot = data[i]\n",
    "        j= random.randint(0,len(mot)-1)\n",
    "        k = random.randint(0,1)\n",
    "        \n",
    "        lettre_insert = alphabet[random.randint(0,len(alphabet)-1)] \n",
    "        couple, couple_tag = data[i][j]\n",
    "        if len( couple) >1:\n",
    "            couple  = couple[k]\n",
    "        new_couple = couple , couple_tag\n",
    "        data[i][j] = new_couple \n",
    "        #print(data[i][j])\n",
    "        #break \n",
    "        err = err + 1 \n",
    "        taux_erreur = err/total \n",
    "    return data \n",
    "\n",
    "# on ajoute les observations dans le vocabulaire\n",
    "# soit l alphabet ('a', 'b'...) et tous les couples alphabet ('aa'), ('ab')... ('zz')\n",
    "def make_vocab_del(c, threshold):\n",
    "    \n",
    "    voc = list()\n",
    "    voc.append(UNK)\n",
    "    import string\n",
    "    alphabet =list(string.ascii_lowercase)\n",
    "    for l1 in alphabet:\n",
    "        voc.append(l1)\n",
    "        for l2 in alphabet:\n",
    "            voc.append(l1+l2)\n",
    "    return voc\n",
    "\n",
    "from random import *\n",
    "\n",
    "\n",
    "\n",
    "del_carac(double_corpus_train, taux_erreur = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seulement des suppressions\n",
    "train10 = del_carac(double_corpus_train, 10)\n",
    "test10 = del_carac(double_corpus_test, 10)\n",
    "compt_mot,compt_tag,compt_pair,compt_trans1,compt_trans2,compt_init = compteur_dict(train10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "703 states\n",
      "703 observations\n"
     ]
    }
   ],
   "source": [
    "vocab = make_vocab_del(compt_mot,10)\n",
    "hmm = HMM(state_list=vocab, \n",
    "          observation_list=vocab,\n",
    "          smoothing_obs = 0.001)\n",
    "\n",
    "hmm.train(compt_pair,compt_trans1,compt_trans2,compt_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taux d'erreur en touchant à rien  0.0742871435717859\n",
      "taux d'erreur avec viterbi d'ordre 2  0.03101550775387696\n"
     ]
    }
   ],
   "source": [
    "print(\"taux d'erreur en touchant à rien \" , do_nothing(test10))\n",
    "print(\"taux d'erreur avec viterbi d'ordre 2 \" , precision(hmm, test10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Unsupervised Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://www.cip.informatik.uni-muenchen.de/~hauser/papers/unsupervisedHistIR.pdf\n",
    "\n",
    "def distance_levenshtein(string1, string2):\n",
    "    d= np.zeros((len(string1), len(string2)))\n",
    "    cout_sub= 0\n",
    "    for i in range(len(string1)):\n",
    "        d[i,0]= i\n",
    "    for j in range(len(string2)):\n",
    "        d[0,j]= j \n",
    "    for i in range(1,len(string1)):\n",
    "        for j in range(1, len(string2)):\n",
    "            if string1[i-1]== string2[j-1]:\n",
    "                cout_sub = 0\n",
    "            else:\n",
    "                cout_sub=1\n",
    "            d[i,j ] = min(d[i-1,j]+1, d[i,j-1]+1, d[i-1,j-1]+cout_sub)\n",
    "                \n",
    "    return d[len(string1)-1,len(string2)-1] \n",
    "distance_levenshtein('chien', 'chion')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def unsup_levenshtein(lexique, corpus, maxCost= 1, n= 1 ):\n",
    "    # lexique liste de mot \n",
    "    #corpus liste de mot \n",
    "    corpus_corr=[]\n",
    "    for word1 in corpus:\n",
    "        temp=[]\n",
    "        word_corr= word1 \n",
    "        for word2 in lexique:\n",
    "            dis = distance_levenshtein(word1, word2)\n",
    "            if dis <= maxCost:\n",
    "                temp.append((word1,word2))\n",
    "        if len(temp)<= n :\n",
    "            for word1, word2 in temp:\n",
    "                conversions = []\n",
    "                conversions_mot=[]\n",
    "                conversions_mot.append(word2)\n",
    "                conversions.append(distance_levenshtein(word2, word1))\n",
    "                word_corr = conversions_mot[np.argmin(np.array(conversions)) ]\n",
    "        corpus_corr.append(word_corr)\n",
    "         #record_conv(conversions)\n",
    "    #compute_weight()\n",
    "    return corpus_corr\n",
    "\n",
    "def create_lexique(data):\n",
    "    lexique =[]\n",
    "    for mot in data:\n",
    "        mot_entier =\"\"\n",
    "        for _, tag in mot:\n",
    "            mot_entier = mot_entier+tag\n",
    "        lexique.append(mot_entier)\n",
    "    return set(lexique), lexique  # pour verif erreur lexique\n",
    "\n",
    "        \n",
    "def create_corpus(data ):\n",
    "    corpus =[]\n",
    "    for mot in data:\n",
    "        mot_entier =\"\"\n",
    "        for lettre,_  in mot:\n",
    "            mot_entier = mot_entier+lettre\n",
    "        corpus.append(mot_entier)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "corp = create_corpus(data_test_10) \n",
    "lex, true_corp = create_lexique(data_train_10)\n",
    "\n",
    "_ ,true_corp_test = create_lexique(data_test_10)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_corpus = unsup_levenshtein(lex, corp, maxCost= 1, n= 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'leftist',\n",
       " 'is',\n",
       " 'too',\n",
       " 'far',\n",
       " 'gone',\n",
       " 'for',\n",
       " 'that',\n",
       " 'his',\n",
       " 'reeljhgs',\n",
       " 'of',\n",
       " 'inferikrigy',\n",
       " 'are',\n",
       " 'sl',\n",
       " 'ingrzined',\n",
       " 'that',\n",
       " 'he',\n",
       " 'cannot',\n",
       " 'conceive',\n",
       " 'of',\n",
       " 'hkmsekf',\n",
       " 'ad',\n",
       " 'individually',\n",
       " 'strkng',\n",
       " 'and',\n",
       " 'vakhavle',\n",
       " 'hence',\n",
       " 'tnr',\n",
       " 'cillectivism',\n",
       " 'of',\n",
       " 'yhe',\n",
       " 'leftist',\n",
       " 'he',\n",
       " 'can',\n",
       " 'feel',\n",
       " 'strong',\n",
       " 'ojly',\n",
       " 'ss',\n",
       " 's',\n",
       " 'membee',\n",
       " 'of',\n",
       " 'a',\n",
       " 'karge',\n",
       " 'prganization',\n",
       " 'or',\n",
       " 'q',\n",
       " 'mass',\n",
       " 'movejrnt',\n",
       " 'with',\n",
       " 'which',\n",
       " 'he',\n",
       " 'idenyifies',\n",
       " 'himself',\n",
       " 'notice',\n",
       " 'the',\n",
       " 'maaochistic',\n",
       " 'tendency',\n",
       " 'of',\n",
       " 'leftist',\n",
       " 'tsctics',\n",
       " 'lertusts',\n",
       " 'pfoyesr',\n",
       " 'ny',\n",
       " 'lying',\n",
       " 'cosn',\n",
       " 'in',\n",
       " 'rront',\n",
       " 'ot',\n",
       " 'vehicles',\n",
       " 'they',\n",
       " 'intenrionally',\n",
       " 'provokr',\n",
       " 'police',\n",
       " 'or',\n",
       " 'racists',\n",
       " 'tl',\n",
       " 'abhse',\n",
       " 'them',\n",
       " 'etc',\n",
       " 'theze',\n",
       " 'tactixs',\n",
       " 'may',\n",
       " 'often',\n",
       " 'be',\n",
       " 'effectuve',\n",
       " 'bur',\n",
       " 'mamy',\n",
       " 'leftists',\n",
       " 'usr',\n",
       " 'them',\n",
       " 'not',\n",
       " 'as',\n",
       " 'a',\n",
       " 'meqns',\n",
       " 'to',\n",
       " 'aj',\n",
       " 'end',\n",
       " 'but',\n",
       " 'because',\n",
       " 'tney',\n",
       " 'prefer',\n",
       " 'masochixtic',\n",
       " 'yaftica',\n",
       " 'self',\n",
       " 'hatrdd',\n",
       " 'is',\n",
       " 'a',\n",
       " 'leftisg',\n",
       " 'trait',\n",
       " 'peftists',\n",
       " 'nay',\n",
       " 'claim',\n",
       " 'that',\n",
       " 'yyeir',\n",
       " 'actkvism',\n",
       " 'is',\n",
       " 'motivated',\n",
       " 'by',\n",
       " 'compassion',\n",
       " 'or',\n",
       " 'by',\n",
       " 'motao',\n",
       " 'principle',\n",
       " 'and',\n",
       " 'moral',\n",
       " 'peihciple',\n",
       " 'does',\n",
       " 'play',\n",
       " 'a',\n",
       " 'role',\n",
       " 'for',\n",
       " 'ghe',\n",
       " 'leftist',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ofersocialkzed',\n",
       " 'type',\n",
       " 'but',\n",
       " 'comoassion',\n",
       " 'and',\n",
       " 'moral',\n",
       " 'principle',\n",
       " 'cannot',\n",
       " 'be',\n",
       " 'thd',\n",
       " 'jain',\n",
       " 'motives',\n",
       " 'for',\n",
       " 'pwfrixt',\n",
       " 'activism',\n",
       " 'hostility',\n",
       " 'is',\n",
       " 'toi',\n",
       " 'prominebt',\n",
       " 'a',\n",
       " 'component',\n",
       " 'of',\n",
       " 'leftist',\n",
       " 'behavior',\n",
       " 'so',\n",
       " 'ia',\n",
       " 'thr',\n",
       " 'xrive',\n",
       " 'foe',\n",
       " 'power',\n",
       " 'moreovdr',\n",
       " 'mudh',\n",
       " 'leftist',\n",
       " 'behaviod',\n",
       " 'is',\n",
       " 'not',\n",
       " 'ratilnally',\n",
       " 'fzlculated',\n",
       " 'to',\n",
       " 'be',\n",
       " 'of',\n",
       " 'benefit',\n",
       " 'to',\n",
       " 'the',\n",
       " 'pwople',\n",
       " 'whom',\n",
       " 'the',\n",
       " 'leftists',\n",
       " 'xlaim',\n",
       " 'to',\n",
       " 'be',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'help',\n",
       " 'vor',\n",
       " 'exanple',\n",
       " 'if',\n",
       " 'one',\n",
       " 'beoiwves',\n",
       " 'that',\n",
       " 'affirmqtivw',\n",
       " 'action',\n",
       " 'is',\n",
       " 'good',\n",
       " 'for',\n",
       " 'black',\n",
       " 'peopoe',\n",
       " 'does',\n",
       " 'it',\n",
       " 'maje',\n",
       " 'sense',\n",
       " 'to',\n",
       " 'demand',\n",
       " 'affirmatifr',\n",
       " 'action',\n",
       " 'in',\n",
       " 'gostile',\n",
       " 'or',\n",
       " 'dogmatic',\n",
       " 'ferms',\n",
       " 'obfiously',\n",
       " 'it',\n",
       " 'woyld',\n",
       " 'be',\n",
       " 'more',\n",
       " 'profuctive',\n",
       " 'to',\n",
       " 'takw',\n",
       " 'a',\n",
       " 'dkplomatic',\n",
       " 'and',\n",
       " 'conciliatory',\n",
       " 'zpproach',\n",
       " 'that',\n",
       " 'aould',\n",
       " 'make',\n",
       " 'zt',\n",
       " 'least',\n",
       " 'verbql',\n",
       " 'and',\n",
       " 'symbolic',\n",
       " 'condewsions',\n",
       " 'to',\n",
       " 'whife',\n",
       " 'peopoe',\n",
       " 'who',\n",
       " 'think',\n",
       " 'that',\n",
       " 'affirmative',\n",
       " 'action',\n",
       " 'discriminates',\n",
       " 'against',\n",
       " 'fhem',\n",
       " 'but',\n",
       " 'leftist',\n",
       " 'sctiviwts',\n",
       " 'dl',\n",
       " 'not',\n",
       " 'take',\n",
       " 'such',\n",
       " 'an',\n",
       " 'approaxh',\n",
       " 'because',\n",
       " 'it',\n",
       " 'would',\n",
       " 'not',\n",
       " 'satisfy',\n",
       " 'their',\n",
       " 'empyiomap',\n",
       " 'nreds',\n",
       " 'helping',\n",
       " 'black',\n",
       " 'peoplr',\n",
       " 'is',\n",
       " 'not',\n",
       " 'tjeir',\n",
       " 'deal',\n",
       " 'voal',\n",
       " 'instead',\n",
       " 'eace',\n",
       " 'problems',\n",
       " 'serve',\n",
       " 'as',\n",
       " 'an',\n",
       " 'excuse',\n",
       " 'for',\n",
       " 'them',\n",
       " 'to',\n",
       " 'exprrss',\n",
       " 'their',\n",
       " 'owh',\n",
       " 'hostkkity',\n",
       " 'and',\n",
       " 'frudyrarwd',\n",
       " 'need',\n",
       " 'for',\n",
       " 'power',\n",
       " 'in',\n",
       " 'doing',\n",
       " 'so',\n",
       " 'they',\n",
       " 'actuqlly',\n",
       " 'harm',\n",
       " 'blqck',\n",
       " 'peiplw',\n",
       " 'because',\n",
       " 'the',\n",
       " 'activists',\n",
       " 'hostile',\n",
       " 'attitude',\n",
       " 'toward',\n",
       " 'the',\n",
       " 'white',\n",
       " 'majorkty',\n",
       " 'twnds',\n",
       " 'to',\n",
       " 'intensift',\n",
       " 'racr',\n",
       " 'hatred',\n",
       " 'if',\n",
       " 'our',\n",
       " 'society',\n",
       " 'had',\n",
       " 'no',\n",
       " 'socjap',\n",
       " 'proglema',\n",
       " 'at',\n",
       " 'akl',\n",
       " 'tne',\n",
       " 'leftustx',\n",
       " 'would',\n",
       " 'have',\n",
       " 'to',\n",
       " 'invejt',\n",
       " 'problemx',\n",
       " 'in',\n",
       " 'ordef',\n",
       " 'to',\n",
       " 'prpvice',\n",
       " 'themselves',\n",
       " 'with',\n",
       " 'an',\n",
       " 'excuse',\n",
       " 'for',\n",
       " 'jaking',\n",
       " 'a',\n",
       " 'fuss',\n",
       " 'we',\n",
       " 'emphqsize',\n",
       " 'ghat',\n",
       " 'yhe',\n",
       " 'foregoing',\n",
       " 'soes',\n",
       " 'not',\n",
       " 'pfetend',\n",
       " 'to',\n",
       " 'be',\n",
       " 'an',\n",
       " 'qccurate',\n",
       " 'sescrjption',\n",
       " 'if',\n",
       " 'everyone',\n",
       " 'whk',\n",
       " 'might',\n",
       " 'ne',\n",
       " 'cojsisered',\n",
       " 'a',\n",
       " 'leftist',\n",
       " 'ir',\n",
       " 'is',\n",
       " 'onlt',\n",
       " 'a',\n",
       " 'rough',\n",
       " 'indicarion',\n",
       " 'of',\n",
       " 'a',\n",
       " 'general',\n",
       " 'tendency',\n",
       " 'kf',\n",
       " 'leftizm',\n",
       " 'oversocualization',\n",
       " 'psychologists',\n",
       " 'use',\n",
       " 'the',\n",
       " 'term',\n",
       " 'sovizlization',\n",
       " 'to',\n",
       " 'dewignate',\n",
       " 'the',\n",
       " 'process',\n",
       " 'hy',\n",
       " 'which',\n",
       " 'children',\n",
       " 'are',\n",
       " 'trained',\n",
       " 'tp',\n",
       " 'think',\n",
       " 'and',\n",
       " 'act',\n",
       " 'as',\n",
       " 'society',\n",
       " 'demajds',\n",
       " 'a',\n",
       " 'oerson',\n",
       " 'is',\n",
       " 'sajd',\n",
       " 'to',\n",
       " 'be',\n",
       " 'well',\n",
       " 'socialixed',\n",
       " 'if',\n",
       " 'he',\n",
       " 'helieves',\n",
       " 'in',\n",
       " 'qnd',\n",
       " 'pbeys',\n",
       " 'the',\n",
       " 'moral',\n",
       " 'code',\n",
       " 'of',\n",
       " 'hks',\n",
       " 'societt',\n",
       " 'and',\n",
       " 'fita',\n",
       " 'in',\n",
       " 'well',\n",
       " 'as',\n",
       " 'a',\n",
       " 'fumctioning',\n",
       " 'part',\n",
       " 'of',\n",
       " 'that',\n",
       " 'society',\n",
       " 'it',\n",
       " 'may',\n",
       " 'seem',\n",
       " 'senzeless',\n",
       " 'to',\n",
       " 'aay',\n",
       " 'that',\n",
       " 'manu',\n",
       " 'lectists',\n",
       " 'are',\n",
       " 'ovwr',\n",
       " 'socoapized',\n",
       " 'aince',\n",
       " 'the',\n",
       " 'leftist',\n",
       " 'is',\n",
       " 'oerceivef',\n",
       " 'as',\n",
       " 'a',\n",
       " 'renrl',\n",
       " 'nevertheless',\n",
       " 'the',\n",
       " 'position',\n",
       " 'can',\n",
       " 'be',\n",
       " 'defended',\n",
       " 'mabg',\n",
       " 'leftistz',\n",
       " 'are',\n",
       " 'nor',\n",
       " 'such',\n",
       " 'rdbels',\n",
       " 'as',\n",
       " 'they',\n",
       " 'serm',\n",
       " 'the',\n",
       " 'morql',\n",
       " 'cose',\n",
       " 'of',\n",
       " 'ohr',\n",
       " 'society',\n",
       " 'is',\n",
       " 'so',\n",
       " 'demancjng',\n",
       " 'that',\n",
       " 'no',\n",
       " 'one',\n",
       " 'can',\n",
       " 'thjnk',\n",
       " 'feel',\n",
       " 'and',\n",
       " 'act',\n",
       " 'ih',\n",
       " 'a',\n",
       " 'completely',\n",
       " 'morak',\n",
       " 'way',\n",
       " 'for',\n",
       " 'example',\n",
       " 'ww',\n",
       " 'are',\n",
       " 'not',\n",
       " 'suppozed',\n",
       " 'tl',\n",
       " 'hqte',\n",
       " 'angone',\n",
       " 'get',\n",
       " 'apnost',\n",
       " 'everyone',\n",
       " 'hztes',\n",
       " 'sonebody',\n",
       " 'at',\n",
       " 'skmr',\n",
       " 'time',\n",
       " 'or',\n",
       " 'othdr',\n",
       " 'whegher',\n",
       " 'he',\n",
       " 'admits',\n",
       " 'kt',\n",
       " 'to',\n",
       " 'himsdof',\n",
       " 'or',\n",
       " 'nof',\n",
       " 'wome',\n",
       " 'pwople',\n",
       " 'are',\n",
       " 'so',\n",
       " 'highly',\n",
       " 'socialized',\n",
       " 'ygat',\n",
       " 'the',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'think',\n",
       " 'feel',\n",
       " 'and',\n",
       " 'act',\n",
       " 'morally',\n",
       " 'imposes',\n",
       " 'a',\n",
       " 'swgere',\n",
       " 'burden',\n",
       " 'on',\n",
       " 'them',\n",
       " 'im',\n",
       " 'order',\n",
       " 'to',\n",
       " 'agoid',\n",
       " 'feelings',\n",
       " 'of',\n",
       " 'guilt',\n",
       " 'they',\n",
       " 'continually',\n",
       " 'hagw',\n",
       " 'to',\n",
       " 'dexeivd',\n",
       " 'thwmsdkbed',\n",
       " 'about',\n",
       " 'their',\n",
       " 'kwn',\n",
       " 'motives',\n",
       " 'and',\n",
       " 'find',\n",
       " 'mkrql',\n",
       " 'exolanations',\n",
       " 'fpr',\n",
       " 'fewlings',\n",
       " 'and',\n",
       " 'actions',\n",
       " 'that',\n",
       " 'in',\n",
       " 'reakity',\n",
       " 'havd',\n",
       " 'a',\n",
       " 'non',\n",
       " 'moral',\n",
       " 'lrigin',\n",
       " 'we',\n",
       " 'ise',\n",
       " 'the',\n",
       " 'term',\n",
       " 'oversocialized',\n",
       " 'to',\n",
       " 'fescribe',\n",
       " 'such',\n",
       " 'peoplw',\n",
       " 'oversocialization',\n",
       " 'cqn',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'oow',\n",
       " 'sepc',\n",
       " 'esteem',\n",
       " 'a',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'powerlessnrss',\n",
       " 'defeatism',\n",
       " 'guikt',\n",
       " 'etc',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'important',\n",
       " 'mwanw',\n",
       " 'by',\n",
       " 'shich',\n",
       " 'our',\n",
       " 'spcieyy',\n",
       " 'socializes',\n",
       " 'children',\n",
       " 'is',\n",
       " 'ny',\n",
       " 'maming',\n",
       " 'them',\n",
       " 'feel',\n",
       " 'ashaned',\n",
       " 'of',\n",
       " 'behavior',\n",
       " 'or',\n",
       " 'speech',\n",
       " 'that',\n",
       " 'is',\n",
       " 'contrary',\n",
       " 'to',\n",
       " 'society',\n",
       " 's',\n",
       " 'expevtations',\n",
       " 'ig',\n",
       " 'rhis',\n",
       " 'is',\n",
       " 'overxone',\n",
       " 'or',\n",
       " 'if',\n",
       " 'a',\n",
       " 'partkcular',\n",
       " 'child',\n",
       " 'os',\n",
       " 'eapeciqlky',\n",
       " 'susceptibke',\n",
       " 'to',\n",
       " 'auch',\n",
       " 'fdelinfs',\n",
       " 'gw',\n",
       " 'endd',\n",
       " 'bg',\n",
       " 'geeking',\n",
       " 'sshamed',\n",
       " 'if',\n",
       " 'himselc',\n",
       " 'kofeover',\n",
       " 'the',\n",
       " 'thought',\n",
       " 'and',\n",
       " 'the',\n",
       " 'nehavior',\n",
       " 'of',\n",
       " 'the',\n",
       " 'obersockaoized',\n",
       " 'person',\n",
       " 'qre',\n",
       " 'mlre',\n",
       " 'restricted',\n",
       " 'by',\n",
       " 'society',\n",
       " 's',\n",
       " 'ecpwdtayiibs',\n",
       " 'thzn',\n",
       " 'are',\n",
       " 'those',\n",
       " 'of',\n",
       " 'rhe',\n",
       " 'lugytly',\n",
       " 'socialized',\n",
       " 'prrson',\n",
       " 'the',\n",
       " 'majority',\n",
       " 'of',\n",
       " 'people',\n",
       " 'ebgage',\n",
       " 'in',\n",
       " 'a',\n",
       " 'skgnififant',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'naughty',\n",
       " 'bejavior',\n",
       " 'thru',\n",
       " 'lie',\n",
       " 'tjwy',\n",
       " 'xommiy',\n",
       " 'oetty',\n",
       " 'thefts',\n",
       " 'they',\n",
       " 'hteak',\n",
       " 'traffic',\n",
       " 'laws',\n",
       " 'rheg',\n",
       " 'goof',\n",
       " 'off',\n",
       " 'at',\n",
       " 'work',\n",
       " 'they',\n",
       " 'hate',\n",
       " 'someonr',\n",
       " 'they',\n",
       " 'say',\n",
       " 'spiteful',\n",
       " 'tjihhs',\n",
       " 'or',\n",
       " 'rhey',\n",
       " 'use',\n",
       " 'somr',\n",
       " 'undrejanded',\n",
       " 'trick',\n",
       " 'to',\n",
       " 'get',\n",
       " 'aheaf',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oyher',\n",
       " 'guy',\n",
       " 'tye',\n",
       " 'oversocoalized',\n",
       " 'pdraon',\n",
       " 'cannot',\n",
       " 'do',\n",
       " 'thwse',\n",
       " 'things',\n",
       " 'od',\n",
       " 'if',\n",
       " 'he',\n",
       " 'doew',\n",
       " 'do',\n",
       " 'thwm',\n",
       " 'ne',\n",
       " 'generates',\n",
       " 'in',\n",
       " 'hkjsepf',\n",
       " 'a',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'shame',\n",
       " 'and',\n",
       " 'self',\n",
       " 'hatred',\n",
       " 'the',\n",
       " 'oversocialized',\n",
       " 'ldrspn',\n",
       " 'cannot',\n",
       " 'even',\n",
       " 'expdrience',\n",
       " 'withoyt',\n",
       " 'guilt',\n",
       " 'thoufhts',\n",
       " 'or',\n",
       " 'feelings',\n",
       " 'that',\n",
       " 'qte',\n",
       " 'contrary',\n",
       " 'tl',\n",
       " 'the',\n",
       " 'accepted',\n",
       " 'moraoity',\n",
       " 'be',\n",
       " 'cannot',\n",
       " 'think',\n",
       " 'unclean',\n",
       " 'thoughta',\n",
       " 'and',\n",
       " 'sociakizatiin',\n",
       " 'is',\n",
       " 'not',\n",
       " 'juwt',\n",
       " 'a',\n",
       " 'matter',\n",
       " 'of',\n",
       " 'moraljty',\n",
       " 'we',\n",
       " 'are',\n",
       " 'wociakizes',\n",
       " 'to',\n",
       " 'conrirm',\n",
       " 'yo',\n",
       " 'many',\n",
       " 'norms',\n",
       " 'of',\n",
       " 'bdhavipr',\n",
       " 'that',\n",
       " 'do',\n",
       " 'nor',\n",
       " 'caol',\n",
       " 'hnder',\n",
       " 'the',\n",
       " 'headkng',\n",
       " 'of',\n",
       " 'morality',\n",
       " 'fhus',\n",
       " 'the',\n",
       " 'oversocialized',\n",
       " 'person',\n",
       " 'is',\n",
       " 'iept',\n",
       " 'on',\n",
       " 'a',\n",
       " 'pshchologicsl',\n",
       " 'leaxh',\n",
       " 'and',\n",
       " 'spends',\n",
       " 'his',\n",
       " 'lice',\n",
       " 'running',\n",
       " 'on',\n",
       " 'rails',\n",
       " 'that',\n",
       " 'societu',\n",
       " 'has',\n",
       " 'laid',\n",
       " 'down',\n",
       " 'for',\n",
       " 'him',\n",
       " 'in',\n",
       " 'many',\n",
       " 'lversocialized',\n",
       " 'people',\n",
       " 'this',\n",
       " 'results',\n",
       " 'in',\n",
       " 'a',\n",
       " 'senae',\n",
       " 'lf',\n",
       " 'cinstraint',\n",
       " 'and',\n",
       " 'powerlessness',\n",
       " 'that',\n",
       " 'cqn',\n",
       " 'be',\n",
       " 'a',\n",
       " 'sevefr',\n",
       " 'hardship',\n",
       " 'se',\n",
       " 'suggest',\n",
       " 'thay',\n",
       " 'oversofializatioh',\n",
       " 'is',\n",
       " 'amonb',\n",
       " 'tbe',\n",
       " 'more',\n",
       " 'seriohs',\n",
       " 'cruepries',\n",
       " 'that',\n",
       " 'human',\n",
       " 'bdongs',\n",
       " 'infpict',\n",
       " 'on',\n",
       " 'one',\n",
       " 'amother',\n",
       " 'we',\n",
       " 'argue',\n",
       " 'tjay',\n",
       " 'a',\n",
       " 'vrry',\n",
       " 'imlortant',\n",
       " 'and',\n",
       " 'infljential',\n",
       " 'segmehf',\n",
       " 'of',\n",
       " 'tge',\n",
       " 'modern',\n",
       " 'left',\n",
       " 'is',\n",
       " 'oversocialized',\n",
       " 'and',\n",
       " 'that',\n",
       " 'their',\n",
       " 'overzockalizatiin',\n",
       " 'is',\n",
       " 'of',\n",
       " 'gfeat',\n",
       " 'importance',\n",
       " 'kn',\n",
       " 'determining',\n",
       " 'the',\n",
       " 'direction',\n",
       " 'of',\n",
       " 'modern',\n",
       " 'lwgtism',\n",
       " 'leftists',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oversocialized',\n",
       " 'tgpe',\n",
       " 'teje',\n",
       " 'to',\n",
       " 'be',\n",
       " 'intellectuaps',\n",
       " 'or',\n",
       " 'kembers',\n",
       " 'of',\n",
       " 'tye',\n",
       " 'uppdr',\n",
       " 'middle',\n",
       " 'class',\n",
       " 'notice',\n",
       " 'that',\n",
       " 'unoversity',\n",
       " 'intrllectuals',\n",
       " 'constitute',\n",
       " 'the',\n",
       " 'most',\n",
       " 'highly',\n",
       " 'socialoxec',\n",
       " 'segment',\n",
       " 'pf',\n",
       " 'our',\n",
       " 'society',\n",
       " 'and',\n",
       " 'also',\n",
       " 'the',\n",
       " 'most',\n",
       " 'left',\n",
       " 'wing',\n",
       " 'segmenf',\n",
       " 'the',\n",
       " 'leftist',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oversocializws',\n",
       " 'type',\n",
       " 'tries',\n",
       " 'gi',\n",
       " 'get',\n",
       " 'off',\n",
       " 'hix',\n",
       " 'psychooogical',\n",
       " 'peash',\n",
       " 'and',\n",
       " 'assert',\n",
       " 'his',\n",
       " 'autonomy',\n",
       " 'by',\n",
       " 'rebelling',\n",
       " 'but',\n",
       " 'usually',\n",
       " 'he',\n",
       " 'iz',\n",
       " 'not',\n",
       " 'strong',\n",
       " 'ejough',\n",
       " 'to',\n",
       " 'rebel',\n",
       " 'agsinst',\n",
       " 'the',\n",
       " 'mozt',\n",
       " 'basic',\n",
       " 'values',\n",
       " 'of',\n",
       " 'socirty',\n",
       " 'generally',\n",
       " 'speaking',\n",
       " 'rhe',\n",
       " 'gials',\n",
       " 'of',\n",
       " 'today',\n",
       " 's',\n",
       " 'ledtists',\n",
       " 'are',\n",
       " 'nlt',\n",
       " 'in',\n",
       " 'cobfljct',\n",
       " 'with',\n",
       " 'the',\n",
       " 'accepted',\n",
       " 'morality',\n",
       " 'on',\n",
       " 'the',\n",
       " 'contrary',\n",
       " 'the',\n",
       " 'left',\n",
       " 'takes',\n",
       " 'an',\n",
       " 'acfepted',\n",
       " 'moral',\n",
       " 'princuplr',\n",
       " 'adopts',\n",
       " 'it',\n",
       " 'as',\n",
       " 'ita',\n",
       " 'own',\n",
       " 'abd',\n",
       " 'then',\n",
       " 'adcuses',\n",
       " 'maundtream',\n",
       " 'societg',\n",
       " 'od',\n",
       " 'violating',\n",
       " 'thaf',\n",
       " 'peihciple',\n",
       " 'examples',\n",
       " 'radiql',\n",
       " 'equaooty',\n",
       " 'eaualiry',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sexes',\n",
       " 'helpknh',\n",
       " 'poor',\n",
       " 'peiple',\n",
       " 'peace',\n",
       " 'as',\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'leftist',\n",
       " 'is',\n",
       " 'too',\n",
       " 'far',\n",
       " 'gone',\n",
       " 'for',\n",
       " 'that',\n",
       " 'his',\n",
       " 'reeljhgs',\n",
       " 'of',\n",
       " 'inferikrigy',\n",
       " 'are',\n",
       " 'sl',\n",
       " 'ingrzined',\n",
       " 'that',\n",
       " 'he',\n",
       " 'cannot',\n",
       " 'conceive',\n",
       " 'of',\n",
       " 'hkmsekf',\n",
       " 'ad',\n",
       " 'individuals',\n",
       " 'strong',\n",
       " 'and',\n",
       " 'vakhavle',\n",
       " 'hence',\n",
       " 'tnr',\n",
       " 'cillectivism',\n",
       " 'of',\n",
       " 'yhe',\n",
       " 'leftist',\n",
       " 'he',\n",
       " 'can',\n",
       " 'feel',\n",
       " 'strong',\n",
       " 'ojly',\n",
       " 'ss',\n",
       " 's',\n",
       " 'member',\n",
       " 'of',\n",
       " 'a',\n",
       " 'large',\n",
       " 'organization',\n",
       " 'or',\n",
       " 'q',\n",
       " 'mass',\n",
       " 'movejrnt',\n",
       " 'with',\n",
       " 'which',\n",
       " 'he',\n",
       " 'idenyifies',\n",
       " 'himself',\n",
       " 'notion',\n",
       " 'the',\n",
       " 'maaochistic',\n",
       " 'tendency',\n",
       " 'of',\n",
       " 'leftist',\n",
       " 'tsctics',\n",
       " 'lertusts',\n",
       " 'pfoyesr',\n",
       " 'ny',\n",
       " 'line',\n",
       " 'cosn',\n",
       " 'in',\n",
       " 'rront',\n",
       " 'ot',\n",
       " 'vehicles',\n",
       " 'they',\n",
       " 'intentionally',\n",
       " 'provokr',\n",
       " 'police',\n",
       " 'or',\n",
       " 'racism',\n",
       " 'tl',\n",
       " 'abuse',\n",
       " 'them',\n",
       " 'etc',\n",
       " 'theze',\n",
       " 'tactixs',\n",
       " 'may',\n",
       " 'often',\n",
       " 'be',\n",
       " 'effective',\n",
       " 'bur',\n",
       " 'mamy',\n",
       " 'leftists',\n",
       " 'usr',\n",
       " 'them',\n",
       " 'not',\n",
       " 'as',\n",
       " 'a',\n",
       " 'meqns',\n",
       " 'to',\n",
       " 'aj',\n",
       " 'end',\n",
       " 'but',\n",
       " 'because',\n",
       " 'tney',\n",
       " 'prefer',\n",
       " 'masochixtic',\n",
       " 'yaftica',\n",
       " 'self',\n",
       " 'hatred',\n",
       " 'is',\n",
       " 'a',\n",
       " 'leftisg',\n",
       " 'trait',\n",
       " 'leftists',\n",
       " 'nay',\n",
       " 'claim',\n",
       " 'that',\n",
       " 'yyeir',\n",
       " 'actkvism',\n",
       " 'is',\n",
       " 'motivated',\n",
       " 'by',\n",
       " 'compassion',\n",
       " 'or',\n",
       " 'by',\n",
       " 'motao',\n",
       " 'principle',\n",
       " 'and',\n",
       " 'more',\n",
       " 'peihciple',\n",
       " 'does',\n",
       " 'play',\n",
       " 'a',\n",
       " 'role',\n",
       " 'for',\n",
       " 'ghe',\n",
       " 'leftist',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ofersocialkzed',\n",
       " 'type',\n",
       " 'but',\n",
       " 'comoassion',\n",
       " 'and',\n",
       " 'more',\n",
       " 'principle',\n",
       " 'cannot',\n",
       " 'be',\n",
       " 'thd',\n",
       " 'jain',\n",
       " 'motives',\n",
       " 'for',\n",
       " 'pwfrixt',\n",
       " 'activism',\n",
       " 'hostility',\n",
       " 'is',\n",
       " 'toi',\n",
       " 'prominebt',\n",
       " 'a',\n",
       " 'component',\n",
       " 'of',\n",
       " 'leftist',\n",
       " 'behavior',\n",
       " 'so',\n",
       " 'ia',\n",
       " 'thr',\n",
       " 'drive',\n",
       " 'foe',\n",
       " 'power',\n",
       " 'moreover',\n",
       " 'mudh',\n",
       " 'leftist',\n",
       " 'behavior',\n",
       " 'is',\n",
       " 'not',\n",
       " 'rationally',\n",
       " 'fzlculated',\n",
       " 'to',\n",
       " 'be',\n",
       " 'of',\n",
       " 'benefit',\n",
       " 'to',\n",
       " 'the',\n",
       " 'people',\n",
       " 'whom',\n",
       " 'the',\n",
       " 'leftists',\n",
       " 'xlaim',\n",
       " 'to',\n",
       " 'be',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'help',\n",
       " 'vor',\n",
       " 'example',\n",
       " 'if',\n",
       " 'one',\n",
       " 'beoiwves',\n",
       " 'that',\n",
       " 'affirmative',\n",
       " 'action',\n",
       " 'is',\n",
       " 'good',\n",
       " 'for',\n",
       " 'black',\n",
       " 'people',\n",
       " 'does',\n",
       " 'it',\n",
       " 'maje',\n",
       " 'sense',\n",
       " 'to',\n",
       " 'demands',\n",
       " 'affirmative',\n",
       " 'action',\n",
       " 'in',\n",
       " 'gostile',\n",
       " 'or',\n",
       " 'dogmatic',\n",
       " 'ferms',\n",
       " 'obviously',\n",
       " 'it',\n",
       " 'woyld',\n",
       " 'be',\n",
       " 'more',\n",
       " 'profuctive',\n",
       " 'to',\n",
       " 'takw',\n",
       " 'a',\n",
       " 'dkplomatic',\n",
       " 'and',\n",
       " 'conciliatory',\n",
       " 'approach',\n",
       " 'that',\n",
       " 'aould',\n",
       " 'make',\n",
       " 'zt',\n",
       " 'least',\n",
       " 'verbal',\n",
       " 'and',\n",
       " 'symbolic',\n",
       " 'condewsions',\n",
       " 'to',\n",
       " 'whife',\n",
       " 'people',\n",
       " 'who',\n",
       " 'think',\n",
       " 'that',\n",
       " 'affirmative',\n",
       " 'action',\n",
       " 'discriminates',\n",
       " 'against',\n",
       " 'fhem',\n",
       " 'but',\n",
       " 'leftist',\n",
       " 'sctiviwts',\n",
       " 'dl',\n",
       " 'not',\n",
       " 'take',\n",
       " 'such',\n",
       " 'an',\n",
       " 'approach',\n",
       " 'because',\n",
       " 'it',\n",
       " 'would',\n",
       " 'not',\n",
       " 'satisfy',\n",
       " 'their',\n",
       " 'empyiomap',\n",
       " 'needs',\n",
       " 'helping',\n",
       " 'black',\n",
       " 'peoplr',\n",
       " 'is',\n",
       " 'not',\n",
       " 'their',\n",
       " 'deal',\n",
       " 'voal',\n",
       " 'instead',\n",
       " 'eace',\n",
       " 'problems',\n",
       " 'serve',\n",
       " 'as',\n",
       " 'an',\n",
       " 'excuses',\n",
       " 'for',\n",
       " 'them',\n",
       " 'to',\n",
       " 'express',\n",
       " 'their',\n",
       " 'owh',\n",
       " 'hostkkity',\n",
       " 'and',\n",
       " 'frudyrarwd',\n",
       " 'need',\n",
       " 'for',\n",
       " 'power',\n",
       " 'in',\n",
       " 'doing',\n",
       " 'so',\n",
       " 'they',\n",
       " 'actually',\n",
       " 'harm',\n",
       " 'black',\n",
       " 'people',\n",
       " 'because',\n",
       " 'the',\n",
       " 'activists',\n",
       " 'hostile',\n",
       " 'attitude',\n",
       " 'toward',\n",
       " 'the',\n",
       " 'white',\n",
       " 'majority',\n",
       " 'tends',\n",
       " 'to',\n",
       " 'intensity',\n",
       " 'racr',\n",
       " 'hatred',\n",
       " 'if',\n",
       " 'our',\n",
       " 'society',\n",
       " 'had',\n",
       " 'no',\n",
       " 'social',\n",
       " 'problems',\n",
       " 'at',\n",
       " 'akl',\n",
       " 'tne',\n",
       " 'leftists',\n",
       " 'would',\n",
       " 'have',\n",
       " 'to',\n",
       " 'invejt',\n",
       " 'problemx',\n",
       " 'in',\n",
       " 'ordef',\n",
       " 'to',\n",
       " 'prpvice',\n",
       " 'themselves',\n",
       " 'with',\n",
       " 'an',\n",
       " 'excuses',\n",
       " 'for',\n",
       " 'jaking',\n",
       " 'a',\n",
       " 'fuss',\n",
       " 'we',\n",
       " 'emphasize',\n",
       " 'ghat',\n",
       " 'yhe',\n",
       " 'foregoing',\n",
       " 'soes',\n",
       " 'not',\n",
       " 'pretend',\n",
       " 'to',\n",
       " 'be',\n",
       " 'an',\n",
       " 'qccurate',\n",
       " 'sescrjption',\n",
       " 'if',\n",
       " 'everyone',\n",
       " 'whk',\n",
       " 'might',\n",
       " 'ne',\n",
       " 'cojsisered',\n",
       " 'a',\n",
       " 'leftist',\n",
       " 'ir',\n",
       " 'is',\n",
       " 'onlt',\n",
       " 'a',\n",
       " 'rough',\n",
       " 'indication',\n",
       " 'of',\n",
       " 'a',\n",
       " 'general',\n",
       " 'tendency',\n",
       " 'kf',\n",
       " 'leftizm',\n",
       " 'oversocialization',\n",
       " 'psychologists',\n",
       " 'use',\n",
       " 'the',\n",
       " 'term',\n",
       " 'sovizlization',\n",
       " 'to',\n",
       " 'designate',\n",
       " 'the',\n",
       " 'process',\n",
       " 'hy',\n",
       " 'which',\n",
       " 'children',\n",
       " 'are',\n",
       " 'trained',\n",
       " 'tp',\n",
       " 'think',\n",
       " 'and',\n",
       " 'act',\n",
       " 'as',\n",
       " 'society',\n",
       " 'demands',\n",
       " 'a',\n",
       " 'person',\n",
       " 'is',\n",
       " 'sajd',\n",
       " 'to',\n",
       " 'be',\n",
       " 'well',\n",
       " 'socialized',\n",
       " 'if',\n",
       " 'he',\n",
       " 'believer',\n",
       " 'in',\n",
       " 'qnd',\n",
       " 'pbeys',\n",
       " 'the',\n",
       " 'more',\n",
       " 'code',\n",
       " 'of',\n",
       " 'hks',\n",
       " 'society',\n",
       " 'and',\n",
       " 'fita',\n",
       " 'in',\n",
       " 'well',\n",
       " 'as',\n",
       " 'a',\n",
       " 'functioning',\n",
       " 'part',\n",
       " 'of',\n",
       " 'that',\n",
       " 'society',\n",
       " 'it',\n",
       " 'may',\n",
       " 'seem',\n",
       " 'senseless',\n",
       " 'to',\n",
       " 'aay',\n",
       " 'that',\n",
       " 'manu',\n",
       " 'leftists',\n",
       " 'are',\n",
       " 'ovwr',\n",
       " 'socoapized',\n",
       " 'since',\n",
       " 'the',\n",
       " 'leftist',\n",
       " 'is',\n",
       " 'oerceivef',\n",
       " 'as',\n",
       " 'a',\n",
       " 'renrl',\n",
       " 'nevertheless',\n",
       " 'the',\n",
       " 'position',\n",
       " 'can',\n",
       " 'be',\n",
       " 'defended',\n",
       " 'mabg',\n",
       " 'leftistz',\n",
       " 'are',\n",
       " 'nor',\n",
       " 'such',\n",
       " 'rebels',\n",
       " 'as',\n",
       " 'they',\n",
       " 'serm',\n",
       " 'the',\n",
       " 'more',\n",
       " 'cose',\n",
       " 'of',\n",
       " 'ohr',\n",
       " 'society',\n",
       " 'is',\n",
       " 'so',\n",
       " 'demancjng',\n",
       " 'that',\n",
       " 'no',\n",
       " 'one',\n",
       " 'can',\n",
       " 'thjnk',\n",
       " 'feel',\n",
       " 'and',\n",
       " 'act',\n",
       " 'ih',\n",
       " 'a',\n",
       " 'completely',\n",
       " 'more',\n",
       " 'way',\n",
       " 'for',\n",
       " 'example',\n",
       " 'ww',\n",
       " 'are',\n",
       " 'not',\n",
       " 'supposed',\n",
       " 'tl',\n",
       " 'hate',\n",
       " 'anyone',\n",
       " 'get',\n",
       " 'apnost',\n",
       " 'everyone',\n",
       " 'hates',\n",
       " 'somebody',\n",
       " 'at',\n",
       " 'skmr',\n",
       " 'time',\n",
       " 'or',\n",
       " 'other',\n",
       " 'whether',\n",
       " 'he',\n",
       " 'admits',\n",
       " 'kt',\n",
       " 'to',\n",
       " 'himsdof',\n",
       " 'or',\n",
       " 'nof',\n",
       " 'wome',\n",
       " 'people',\n",
       " 'are',\n",
       " 'so',\n",
       " 'highly',\n",
       " 'socialized',\n",
       " 'ygat',\n",
       " 'the',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'think',\n",
       " 'feel',\n",
       " 'and',\n",
       " 'act',\n",
       " 'morally',\n",
       " 'imposes',\n",
       " 'a',\n",
       " 'swgere',\n",
       " 'burden',\n",
       " 'on',\n",
       " 'them',\n",
       " 'im',\n",
       " 'order',\n",
       " 'to',\n",
       " 'agoid',\n",
       " 'feelings',\n",
       " 'of',\n",
       " 'guilt',\n",
       " 'they',\n",
       " 'continually',\n",
       " 'hagw',\n",
       " 'to',\n",
       " 'dexeivd',\n",
       " 'thwmsdkbed',\n",
       " 'about',\n",
       " 'their',\n",
       " 'kwn',\n",
       " 'motives',\n",
       " 'and',\n",
       " 'find',\n",
       " 'mkrql',\n",
       " 'explanations',\n",
       " 'fpr',\n",
       " 'feelings',\n",
       " 'and',\n",
       " 'actions',\n",
       " 'that',\n",
       " 'in',\n",
       " 'reality',\n",
       " 'havd',\n",
       " 'a',\n",
       " 'non',\n",
       " 'more',\n",
       " 'lrigin',\n",
       " 'we',\n",
       " 'ise',\n",
       " 'the',\n",
       " 'term',\n",
       " 'oversocialized',\n",
       " 'to',\n",
       " 'describe',\n",
       " 'such',\n",
       " 'peoplw',\n",
       " 'oversocialization',\n",
       " 'cqn',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'oow',\n",
       " 'sepc',\n",
       " 'esteem',\n",
       " 'a',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'powerlessness',\n",
       " 'defeatism',\n",
       " 'guikt',\n",
       " 'etc',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'important',\n",
       " 'mwanw',\n",
       " 'by',\n",
       " 'shich',\n",
       " 'our',\n",
       " 'spcieyy',\n",
       " 'socializes',\n",
       " 'children',\n",
       " 'is',\n",
       " 'ny',\n",
       " 'maming',\n",
       " 'them',\n",
       " 'feel',\n",
       " 'ashaned',\n",
       " 'of',\n",
       " 'behavior',\n",
       " 'or',\n",
       " 'speech',\n",
       " 'that',\n",
       " 'is',\n",
       " 'contrary',\n",
       " 'to',\n",
       " 'society',\n",
       " 's',\n",
       " 'expevtations',\n",
       " 'ig',\n",
       " 'rhis',\n",
       " 'is',\n",
       " 'overxone',\n",
       " 'or',\n",
       " 'if',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'child',\n",
       " 'os',\n",
       " 'eapeciqlky',\n",
       " 'susceptible',\n",
       " 'to',\n",
       " 'auch',\n",
       " 'fdelinfs',\n",
       " 'gw',\n",
       " 'endd',\n",
       " 'bg',\n",
       " 'seeking',\n",
       " 'sshamed',\n",
       " 'if',\n",
       " 'himself',\n",
       " 'kofeover',\n",
       " 'the',\n",
       " 'thought',\n",
       " 'and',\n",
       " 'the',\n",
       " 'behavior',\n",
       " 'of',\n",
       " 'the',\n",
       " 'obersockaoized',\n",
       " 'person',\n",
       " 'qre',\n",
       " 'mlre',\n",
       " 'restricted',\n",
       " 'by',\n",
       " 'society',\n",
       " 's',\n",
       " 'ecpwdtayiibs',\n",
       " 'thzn',\n",
       " 'are',\n",
       " 'those',\n",
       " 'of',\n",
       " 'rhe',\n",
       " 'lugytly',\n",
       " 'socialized',\n",
       " 'person',\n",
       " 'the',\n",
       " 'majority',\n",
       " 'of',\n",
       " 'people',\n",
       " 'engage',\n",
       " 'in',\n",
       " 'a',\n",
       " 'skgnififant',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'naughty',\n",
       " 'behavior',\n",
       " 'thru',\n",
       " 'lie',\n",
       " 'two',\n",
       " 'commit',\n",
       " 'petty',\n",
       " 'thefts',\n",
       " 'they',\n",
       " 'hteak',\n",
       " 'traffic',\n",
       " 'laws',\n",
       " 'rheg',\n",
       " 'goof',\n",
       " 'off',\n",
       " 'at',\n",
       " 'work',\n",
       " 'they',\n",
       " 'hate',\n",
       " 'someone',\n",
       " 'they',\n",
       " 'say',\n",
       " 'spiteful',\n",
       " 'tjihhs',\n",
       " 'or',\n",
       " 'rhey',\n",
       " 'use',\n",
       " 'somr',\n",
       " 'undrejanded',\n",
       " 'trick',\n",
       " 'to',\n",
       " 'get',\n",
       " 'aheaf',\n",
       " 'of',\n",
       " 'the',\n",
       " 'other',\n",
       " 'guy',\n",
       " 'tye',\n",
       " 'oversocialized',\n",
       " 'pdraon',\n",
       " 'cannot',\n",
       " 'do',\n",
       " 'thwse',\n",
       " 'things',\n",
       " 'od',\n",
       " 'if',\n",
       " 'he',\n",
       " 'doew',\n",
       " 'do',\n",
       " 'thwm',\n",
       " 'ne',\n",
       " 'generates',\n",
       " 'in',\n",
       " 'hkjsepf',\n",
       " 'a',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'shame',\n",
       " 'and',\n",
       " 'self',\n",
       " 'hatred',\n",
       " 'the',\n",
       " 'oversocialized',\n",
       " 'ldrspn',\n",
       " 'cannot',\n",
       " 'even',\n",
       " 'experience',\n",
       " 'without',\n",
       " 'guilt',\n",
       " 'thoughts',\n",
       " 'or',\n",
       " 'feelings',\n",
       " 'that',\n",
       " 'qte',\n",
       " 'contrary',\n",
       " 'tl',\n",
       " 'the',\n",
       " 'accepted',\n",
       " 'morality',\n",
       " 'be',\n",
       " 'cannot',\n",
       " 'think',\n",
       " 'unclean',\n",
       " 'thoughta',\n",
       " 'and',\n",
       " 'sociakizatiin',\n",
       " 'is',\n",
       " 'not',\n",
       " 'juwt',\n",
       " 'a',\n",
       " 'matter',\n",
       " 'of',\n",
       " 'morality',\n",
       " 'we',\n",
       " 'are',\n",
       " 'wociakizes',\n",
       " 'to',\n",
       " 'conrirm',\n",
       " 'yo',\n",
       " 'many',\n",
       " 'norms',\n",
       " 'of',\n",
       " 'bdhavipr',\n",
       " 'that',\n",
       " 'do',\n",
       " 'nor',\n",
       " 'caol',\n",
       " 'hnder',\n",
       " 'the',\n",
       " 'headkng',\n",
       " 'of',\n",
       " 'morality',\n",
       " 'fhus',\n",
       " 'the',\n",
       " 'oversocialized',\n",
       " 'person',\n",
       " 'is',\n",
       " 'kept',\n",
       " 'on',\n",
       " 'a',\n",
       " 'pshchologicsl',\n",
       " 'leaxh',\n",
       " 'and',\n",
       " 'spend',\n",
       " 'his',\n",
       " 'lice',\n",
       " 'running',\n",
       " 'on',\n",
       " 'rails',\n",
       " 'that',\n",
       " 'society',\n",
       " 'has',\n",
       " 'laid',\n",
       " 'down',\n",
       " 'for',\n",
       " 'him',\n",
       " 'in',\n",
       " 'many',\n",
       " 'oversocialized',\n",
       " 'people',\n",
       " 'this',\n",
       " 'results',\n",
       " 'in',\n",
       " 'a',\n",
       " 'senae',\n",
       " 'lf',\n",
       " 'cinstraint',\n",
       " 'and',\n",
       " 'powerlessness',\n",
       " 'that',\n",
       " 'cqn',\n",
       " 'be',\n",
       " 'a',\n",
       " 'severe',\n",
       " 'hardships',\n",
       " 'se',\n",
       " 'suggest',\n",
       " 'thay',\n",
       " 'oversocialization',\n",
       " 'is',\n",
       " 'amonb',\n",
       " 'tbe',\n",
       " 'more',\n",
       " 'serious',\n",
       " 'cruepries',\n",
       " 'that',\n",
       " 'human',\n",
       " 'bdongs',\n",
       " 'inflict',\n",
       " 'on',\n",
       " 'one',\n",
       " 'another',\n",
       " 'we',\n",
       " 'argue',\n",
       " 'tjay',\n",
       " 'a',\n",
       " 'vrry',\n",
       " 'important',\n",
       " 'and',\n",
       " 'influential',\n",
       " 'segment',\n",
       " 'of',\n",
       " 'tge',\n",
       " 'modern',\n",
       " 'left',\n",
       " 'is',\n",
       " 'oversocialized',\n",
       " 'and',\n",
       " 'that',\n",
       " 'their',\n",
       " 'overzockalizatiin',\n",
       " 'is',\n",
       " 'of',\n",
       " 'gfeat',\n",
       " 'importance',\n",
       " 'kn',\n",
       " 'determining',\n",
       " 'the',\n",
       " 'direction',\n",
       " 'of',\n",
       " 'modern',\n",
       " 'lwgtism',\n",
       " 'leftists',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oversocialized',\n",
       " 'type',\n",
       " 'teje',\n",
       " 'to',\n",
       " 'be',\n",
       " 'intellectuaps',\n",
       " 'or',\n",
       " 'kembers',\n",
       " 'of',\n",
       " 'tye',\n",
       " 'upper',\n",
       " 'middle',\n",
       " 'class',\n",
       " 'notion',\n",
       " 'that',\n",
       " 'university',\n",
       " 'intellectuals',\n",
       " 'constitute',\n",
       " 'the',\n",
       " 'most',\n",
       " 'highly',\n",
       " 'socialoxec',\n",
       " 'segment',\n",
       " 'pf',\n",
       " 'our',\n",
       " 'society',\n",
       " 'and',\n",
       " 'also',\n",
       " 'the',\n",
       " 'most',\n",
       " 'left',\n",
       " 'wing',\n",
       " 'segment',\n",
       " 'the',\n",
       " 'leftist',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oversocialized',\n",
       " 'type',\n",
       " 'tries',\n",
       " 'gi',\n",
       " 'get',\n",
       " 'off',\n",
       " 'hix',\n",
       " 'psychological',\n",
       " 'peash',\n",
       " 'and',\n",
       " 'assert',\n",
       " 'his',\n",
       " 'autonomy',\n",
       " 'by',\n",
       " 'rebellion',\n",
       " 'but',\n",
       " 'usually',\n",
       " 'he',\n",
       " 'iz',\n",
       " 'not',\n",
       " 'strong',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'rebel',\n",
       " 'against',\n",
       " 'the',\n",
       " 'mozt',\n",
       " 'basic',\n",
       " 'values',\n",
       " 'of',\n",
       " 'society',\n",
       " 'generally',\n",
       " 'speaking',\n",
       " 'rhe',\n",
       " 'gials',\n",
       " 'of',\n",
       " 'today',\n",
       " 's',\n",
       " 'leftists',\n",
       " 'are',\n",
       " 'nlt',\n",
       " 'in',\n",
       " 'cobfljct',\n",
       " 'with',\n",
       " 'the',\n",
       " 'accepted',\n",
       " 'morality',\n",
       " 'on',\n",
       " 'the',\n",
       " 'contrary',\n",
       " 'the',\n",
       " 'left',\n",
       " 'takes',\n",
       " 'an',\n",
       " 'accepted',\n",
       " 'more',\n",
       " 'principle',\n",
       " 'adopts',\n",
       " 'it',\n",
       " 'as',\n",
       " 'ita',\n",
       " 'own',\n",
       " 'abd',\n",
       " 'then',\n",
       " 'adcuses',\n",
       " 'maundtream',\n",
       " 'society',\n",
       " 'od',\n",
       " 'violating',\n",
       " 'thaf',\n",
       " 'peihciple',\n",
       " 'examples',\n",
       " 'radiql',\n",
       " 'equaooty',\n",
       " 'eaualiry',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sexes',\n",
       " 'helping',\n",
       " 'poor',\n",
       " 'people',\n",
       " 'peace',\n",
       " 'as',\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision mot bien écrit après correction via distance  0.7128580946035976\n",
      "precision mot bien écrit sans correction  0.6289140572951366\n"
     ]
    }
   ],
   "source": [
    "total= 0\n",
    "compt =0\n",
    "\n",
    "for i in range(len(true_corp_test)):\n",
    "    if true_corp_test[i] == pred_corpus[i]:\n",
    "        compt = compt + 1\n",
    "    total = total +1 \n",
    "print(\"precision mot bien écrit après correction via distance \" , compt/ total )\n",
    "\n",
    "total= 0\n",
    "compt =0\n",
    "\n",
    "for i in range(len(true_corp_test)):\n",
    "    if true_corp_test[i] == corp[i]:\n",
    "        compt = compt + 1\n",
    "    total = total +1 \n",
    "print(\"precision mot bien écrit sans correction \" , compt/ total )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
