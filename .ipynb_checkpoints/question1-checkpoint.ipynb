{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IwILkW1F8FnJ"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Ce TP continue le TP précédent. Nous allons reprendre d'ailleurs les mêmes données et commencer la mise en oeuvre d'un modèle de Markov pour la prédiction des étiquettes: \n",
    "* une observation est une phrase, représentée comme une séquence de variables aléatoires, une par mot de la phrase\n",
    "* à cette observation est associée une séquence de variables aléatoires représentant les étiquettes, une par mot de la phrase également\n",
    "\n",
    "On suppose que la séquence d'observation (une phrase) est générée par un modèle de Markov caché. Les variables cachées sont donc les étiquettes à inférer. Nous allons commencer par écrire une classe python pour représenter le HMM. Cette classe évoluera au fil des TPs. \n",
    "\n",
    "Pour cela le code de départ suivant est donné. Afin d'initialiser un HMM, nous devons connaitre : \n",
    "- l'ensemble des états (ou *state_list*), dans notre cas l'ensemble des étiquettes grammaticales;\n",
    "- l'ensemble des observations (ou *observation_list*), dans notre cas l'ensemble des mots connus; tous les autres mots seront remplacés par l'élément spécial *UNK* qui fait partie de l'ensemble des observations. \n",
    "\n",
    "Enfin, en interne il est plus facile d'indexer les mots et et les états par des entiers. Ainsi à chaque éléments de respectivement l'ensemble des états et l'ensemble des observations, est associé un indice. Cela nous permet de tout traiter en \"matricielle\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train20.pkl', 'rb') as f:\n",
    "#with open('train10.pkl', 'rb') as f:\n",
    "    data_train = pickle.load(f)\n",
    "with open('test20.pkl', 'rb') as f:    \n",
    "#with open('test10.pkl', 'rb') as f:\n",
    "    data_test = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27184"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NtP9d0Pz8FnL"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros\n",
    "import sys\n",
    "\n",
    "# Some words in test could be unseen during training, or out of the vocabulary (OOV) even during the training. \n",
    "# To manage OOVs, all words out the vocabulary are mapped on a special token: UNK defined as follows: \n",
    "UNK = \"<unk>\" \n",
    "UNKid = 0 \n",
    "\n",
    "class HMM:\n",
    "        def __init__(self, state_list, observation_list,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None,\n",
    "                    transition_proba_2= None):\n",
    "            \"\"\"Builds a new Hidden Markov Model\n",
    "            state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "            observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "            transition_proba is the transition probability matrix\n",
    "                [a_ij] a_ij = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            observation_proba is the observation probablility matrix\n",
    "                [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "            initial_state_proba is the initial state distribution\n",
    "                [pi_i] pi_i = Pr(Y_0=q_i)\"\"\"\n",
    "            print (\"HMM creating with: \")\n",
    "            self.N = len(state_list) # The number of states\n",
    "            self.M = len(observation_list) # The number of words in the vocabulary\n",
    "            print (str(self.N)+\" states\")\n",
    "            print (str(self.M)+\" observations\")\n",
    "            self.omega_Y = state_list # Keep the vocabulary of tags\n",
    "            self.omega_X = observation_list # Keep the vocabulary of tags\n",
    "            # Init. of the 3 distributions : observation, transition and initial states\n",
    "            if transition_proba is None:\n",
    "                self.transition_proba = zeros( (self.N, self.N), float) \n",
    "            else:\n",
    "                self.transition_proba=transition_proba\n",
    "                \n",
    "            if transition_proba_2 is None:\n",
    "                self.transition_proba_2 = zeros( (self.N, self.N**2), float) \n",
    "            else:\n",
    "                self.transition_proba_2=transition_proba_2   \n",
    "                \n",
    "            if observation_proba is None:\n",
    "                self.observation_proba = zeros( (self.M, self.N), float) \n",
    "            else:\n",
    "                self.observation_proba=observation_proba\n",
    "            if initial_state_proba is None:\n",
    "                self.initial_state_proba = zeros( (self.N,), float ) \n",
    "            else:\n",
    "                self.initial_state_proba=initial_state_proba\n",
    "            # Since everything will be stored in numpy arrays, it is more convenient and compact to \n",
    "            # handle words and tags as indices (integer) for a direct access. However, we also need \n",
    "            # to keep the mapping between strings (word or tag) and indices. \n",
    "            self.make_indexes()\n",
    "            \n",
    "        def make_indexes(self):\n",
    "            \"\"\"Creates the reverse table that maps states/observations names\n",
    "                to their index in the probabilities array\"\"\"\n",
    "            self.Y_index = {}\n",
    "            for i in range(self.N):\n",
    "                \n",
    "                self.Y_index[list(self.omega_Y)[i]] = i\n",
    "            self.X_index = {}\n",
    "            for i in range(self.M):\n",
    "                self.X_index[list(self.omega_X)[i]] = i\n",
    "\n",
    "\n",
    "       # Matrice de transitions (N, N)\n",
    "        def transition_matrix(self , compt_transition, compt_tag):\n",
    "            trans = np.zeros((len(compt_tag), len(compt_tag)))\n",
    "            N= len(compt_tag)\n",
    "            i=0\n",
    "            j=0\n",
    "            for el1, value in compt_tag.items():\n",
    "                for el2, value in compt_tag.items():\n",
    "                    try:\n",
    "                        trans[i,j]= compt_transition[(el1, el2)]\n",
    "                    except:\n",
    "                        trans[i,j]=0\n",
    "                        \n",
    "                    j=j+1\n",
    "                i=i+1\n",
    "                j=0\n",
    "            trans = trans + 0.0001\n",
    "            trans= trans/ np.sum(trans,axis=0)\n",
    "            self.transition_proba = trans\n",
    "           # return trans\n",
    "        \n",
    "        # Mots et le tag associés (M, N)\n",
    "        def observation_matrix(self, compt_tag, compt_pair, compt_mot):\n",
    "            obs = np.zeros((len(compt_pair), len(compt_tag)))\n",
    "            i=0\n",
    "            j=0\n",
    "            for word, value  in compt_mot.items():\n",
    "                for tags, value in compt_tag.items():\n",
    "                    try:\n",
    "                        obs[i,j]=compt_pair[(word, tags)]\n",
    "                    except:\n",
    "                        obs[i,j]=0\n",
    "                    j=j+1\n",
    "                i=i+1\n",
    "                j=0\n",
    "            obs = obs + 0.0001\n",
    "            obs = obs/ np.sum(obs,axis=0)\n",
    "            self.observation_proba = obs\n",
    "           # return obs\n",
    "        \n",
    "        def observation_matrix(self, compt_tag, compt_pair, compt_mot):\n",
    "            obs = np.zeros((len(compt_pair), len(compt_tag)))\n",
    "            i=0\n",
    "            j=0\n",
    "            l= []\n",
    "            for word, value  in compt_mot.items():\n",
    "                for tags, value in compt_tag.items():\n",
    "                    try:\n",
    "                        obs[i,j]=compt_pair[(word, tags)]\n",
    "                    except:\n",
    "                        obs[i,j]=0\n",
    "                    j=j+1\n",
    "                i=i+1\n",
    "                j=0\n",
    "            obs = obs[:len(compt_mot),:]\n",
    "            obs = obs/ np.sum(obs,axis=0)\n",
    "            self.observation_proba = obs\n",
    "           # return obs\n",
    "        \n",
    "        \n",
    "        # Inititations des tags (N,)\n",
    "        def init(self, compt_init):\n",
    "            init= np.zeros(len(compt_init))\n",
    "            i=0\n",
    "            for tag, valeur in compt_init.items():\n",
    "                init[i] = valeur\n",
    "                i=i+1\n",
    "            init = init +0.0001\n",
    "            init = init/np.sum(init)\n",
    "            \n",
    "            self.initial_state_proba = init \n",
    "           # return init\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        def matrix_transition3(self , compt_trans2,compt_transition):\n",
    "            import string\n",
    "            alphabet =list(string.ascii_lowercase)\n",
    "            alpha_t = alpha_tuple(alphabet)\n",
    "            trans_2 = np.zeros((len(alphabet), len(alpha_t)))\n",
    "            N= len(alphabet)\n",
    "            i=0\n",
    "            j=0\n",
    "            k=0\n",
    "            list_1=[]\n",
    "            for l in alphabet :\n",
    "                for l_1, l_2 in alpha_t:\n",
    "\n",
    "                    try: \n",
    "                        trans_2[i,j]= compt_trans2_train[((l_1,l),(l_2,l)) ]  /compt_transition[(l_2,l_1)]\n",
    "                    except:\n",
    "\n",
    "                        trans_2[i,j]=0\n",
    "                    j=j+1\n",
    "\n",
    "                i=i+1\n",
    "                j=0\n",
    "            trans_2 = trans_2 + 0.0001\n",
    "            trans_2 = trans_2 / trans_2.sum(axis=0).reshape(1, self.N**2)\n",
    "            self.transition_proba_2 = trans_2\n",
    "        \n",
    "        def train(self,  compt_tag, compt_pair, compt_mot, compt_init, compt_transition, compt_transition_2 = None ):\n",
    "            self.observation_matrix(compt_tag, compt_pair, compt_mot)\n",
    "            self.transition_matrix(compt_transition, compt_tag)\n",
    "            self.init(compt_init)\n",
    "            if compt_transition_2 !=None:\n",
    "                self.matrix_transition3(compt_transition_2,compt_transition)\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compter les mots, les tags , les paires , les transitions , les premiers mots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fonction qui compte mots, tags, pairS (mot, tag), les transitions(tag1, tag2), les premiers mots \n",
    "# compter : lettre (x ) , lettre (y), (pair: (x, y) (lettre, lettre ), (etat, etat): (lettre, lettre ), (lettre premiere:  etat)\n",
    "\n",
    "def compteur_dict(data_sent):\n",
    "\n",
    "    compt_mot = {}\n",
    "    compt_tag = {}\n",
    "    compt_pair= {}\n",
    "    compt_transition = {}\n",
    "    compt_init = {}\n",
    "    compt_transitions2 ={}\n",
    "    \n",
    "    \n",
    "    for sentence in data_sent:\n",
    "        for i in range(len(sentence)):\n",
    "            pair=sentence[i]\n",
    "            mot, tag =pair \n",
    "            \n",
    "            if mot in compt_mot:\n",
    "                compt_mot[mot]=compt_mot[mot]+1\n",
    "            else:\n",
    "                compt_mot[mot]=1\n",
    "                \n",
    "            if tag in compt_tag:\n",
    "                compt_tag[tag]=compt_tag[tag]+1\n",
    "            else:\n",
    "                compt_tag[tag]=1\n",
    "                \n",
    "            if pair in compt_pair:\n",
    "                compt_pair[pair]=compt_pair[pair]+1\n",
    "            else:\n",
    "                compt_pair[pair]=1\n",
    "                \n",
    "            if i > 1:\n",
    "                trans2 = ((sentence[i-2][1], sentence[i-1][1]), tag) #(tag at t-2, tag at t-1, tag at t)\n",
    "                if trans2 in c_transitions2:\n",
    "                    compt_transitions2[trans2] = compt_transitions2[trans2] + 1\n",
    "                else:\n",
    "                    compt_transitions2[trans2] = 1\n",
    "\n",
    "            if i > 0:\n",
    "                trans = (sentence[i-1][1],tag)\n",
    "                if trans in compt_transition:\n",
    "                    compt_transition[trans]=compt_transition[trans]+1\n",
    "                else:\n",
    "                    compt_transition[trans]=1\n",
    "                    \n",
    "            else:\n",
    "                if tag in compt_init:\n",
    "                    compt_init[tag]=compt_init[tag]+1\n",
    "                else:\n",
    "                    compt_init[tag]=1\n",
    "                    \n",
    "    return compt_mot,compt_tag,compt_pair, compt_transition, compt_init, compt_transitions2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rajoutez z = 0\n",
    "\n",
    "compt_mot_train,compt_tag_train,compt_pair_train, compt_transition_train, compt_init_train, compt_transition2_train =compteur_dict(data_train)\n",
    "compt_init_train['z']=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation du train/ test set + filtrage vocabulaire >10 occurences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "26 states\n",
      "26 observations\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(state_list=compt_tag_train.keys(), observation_list=compt_mot_train.keys(),\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None,\n",
    "                 transition_proba_2 =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "hmm.train(compt_tag_train, compt_pair_train, compt_mot_train, compt_init_train, compt_transition_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition matrix  (26, 26)\n",
      "observation matrix  (26, 26)\n",
      "initiation matrix  (26,)\n",
      "transition2 matrix (26, 676)\n"
     ]
    }
   ],
   "source": [
    "print(\"transition matrix \", hmm.transition_proba.shape)\n",
    "print(\"observation matrix \", hmm.observation_proba.shape)\n",
    "print(\"initiation matrix \", hmm.initial_state_proba.shape)\n",
    "print(\"transition2 matrix\", hmm.transition_proba_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(hmm.transition_proba, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper / Demapper vers les observations / tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utilisable pour tag \n",
    "\n",
    "def encode_word(sentence_tokenize, words):\n",
    "    enc=[]\n",
    "    for word, tag in sentence_tokenize:\n",
    "        i=0\n",
    "        for w in words:\n",
    "            if w == word:\n",
    "                enc.append(i)\n",
    "            i=i+1\n",
    "        \n",
    "    return enc\n",
    "\n",
    "\n",
    "\n",
    "def decode_word(numbers, words):\n",
    "    dec=[]\n",
    "    for num in numbers:\n",
    "        for i in range(0,len(words)):\n",
    "            if i==num:\n",
    "                dec.append(words[i])\n",
    "            i=i+1\n",
    "    return dec \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mot initial [('a', 'a'), ('c', 'c'), ('f', 'c'), ('o', 'o'), ('u', 'u'), ('n', 'n'), ('f', 't')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a', 'c', 't', 'o', 'u', 'n', 'f']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def viterbi(sentence, trans, obs, init):\n",
    "   \n",
    "    N = trans.shape[0]\n",
    "    \n",
    "    init = init if init is not None else np.full(N, 1 / N)\n",
    "    T = len(sentence)\n",
    "    T1 = np.empty((N, T), 'd')\n",
    "    T2 = np.empty((N, T), 'B')\n",
    "\n",
    "   \n",
    "    T1[:, 0] = init * obs[:, sentence[0]]\n",
    "    T2[:, 0] = 0\n",
    "\n",
    "    for i in range(1, T):\n",
    "        T1[:, i] = np.max(T1[:, i - 1] * trans.T * obs[np.newaxis, :, sentence[i]].T, 1)\n",
    "        T2[:, i] = np.argmax(T1[:, i - 1] * trans.T, 1)\n",
    "\n",
    "    x = np.empty(T, 'B')\n",
    "    x[-1] = np.argmax(T1[:, T - 1])\n",
    "    for i in reversed(range(1, T)):\n",
    "        x[i - 1] = T2[x[i], i]\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "y=  encode_word(data_train[3], compt_mot_train )\n",
    "\n",
    "\n",
    "trans, obs , ini = hmm.transition_proba, hmm.observation_proba, hmm.initial_state_proba\n",
    "x = viterbi(  y, trans, obs.T, ini)\n",
    "\n",
    "print(\"mot initial\", data_train[3])\n",
    " # x = predictions tag : indice des tags prédit dans le dictionnaire  \n",
    "decode_word(x, list(compt_tag_train.keys()))  #recupère les tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associer predictions aux mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# L'état 0 ne renvoit pas de prédictions , si mot pas dans vocaublaire =>chunk, unkid = 0\n",
    "def associate_word_tag(data_sent, vocabulaire, tag ,  trans, obs, ini ):\n",
    "    result =[]\n",
    "    data_set = []\n",
    "    for line  in data_sent:\n",
    "        \n",
    "       \n",
    "        y=  encode_word(line, vocabulaire ) #ex :['Bonjour', 'le', 'monde'] = >[3,8,11]  en fonction emplacement dans dictionnaire\n",
    "        \n",
    "        # si ligne vide ou aucun mot de la ligne contenu dans le vocabulaire\n",
    "        \n",
    "        \n",
    "        if y == []:\n",
    "            print(line)\n",
    "            for val in range(0, len(line)):\n",
    "                result.append(('CHUNK' , 0))\n",
    "                data_set.append(line[val] )\n",
    "                \n",
    "        if y != []:\n",
    "            # x = predictions index de tag, en fonction emplacement dans dictionnaire\n",
    "\n",
    "            x = viterbi(  y, trans, obs.T, ini)\n",
    "\n",
    "            tag_pred = decode_word(x, list(tag))   # cherche dans le dictionnaire tags correspondant aux index\n",
    "\n",
    "            for j in range(0, len(tag_pred)):\n",
    "                if j == 0 :\n",
    "                    i=0\n",
    "                while line[i][0]  not in vocabulaire and i <len(line):\n",
    "                    result.append(('CHUNK' , 0))\n",
    "                    data_set.append(line[i] )\n",
    "                    i=i+1\n",
    "\n",
    "                if line[i][0] in vocabulaire:\n",
    "                    result.append((line[i][0] , tag_pred[j]))\n",
    "                    data_set.append(line[i] )\n",
    "                i=i+1\n",
    "    return result , data_set\n",
    "\n",
    "\n",
    "predictions =associate_word_tag(data_train[0:1000] , compt_mot_train, compt_tag_train.keys() ,  trans, obs, ini )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855 4855\n"
     ]
    }
   ],
   "source": [
    "result , data_set = predictions\n",
    "print(len(result), len(data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('w', 's'),\n",
       "  ('i', 'i'),\n",
       "  ('g', 'g'),\n",
       "  ('n', 'n'),\n",
       "  ('i', 'i'),\n",
       "  ('t', 'f'),\n",
       "  ('i', 'i'),\n",
       "  ('c', 'c'),\n",
       "  ('a', 'a'),\n",
       "  ('n', 'n'),\n",
       "  ('f', 't')],\n",
       " [('q', 'q'),\n",
       "  ('u', 'u'),\n",
       "  ('w', 'e'),\n",
       "  ('s', 's'),\n",
       "  ('y', 't'),\n",
       "  ('i', 'i'),\n",
       "  ('o', 'o'),\n",
       "  ('m', 'n')],\n",
       " [('t', 't'), ('o', 'o')],\n",
       " [('w', 'w'), ('h', 'h'), ('i', 'i'), ('c', 'c'), ('h', 'h')],\n",
       " [('h', 'h'),\n",
       "  ('i', 'i'),\n",
       "  ('d', 's'),\n",
       "  ('t', 't'),\n",
       "  ('o', 'o'),\n",
       "  ('r', 'r'),\n",
       "  ('j', 'i'),\n",
       "  ('s', 'a'),\n",
       "  ('n', 'n'),\n",
       "  ('d', 's')],\n",
       " [('o', 'o'), ('i', 'u'), ('g', 'g'), ('h', 'h'), ('t', 't')],\n",
       " [('t', 't'), ('k', 'o')],\n",
       " [('g', 'g'), ('i', 'i'), ('f', 'v'), ('e', 'e')],\n",
       " [('t', 't'), ('n', 'h'), ('r', 'e'), ('i', 'i'), ('r', 'r')],\n",
       " [('a', 'a'),\n",
       "  ('t', 't'),\n",
       "  ('t', 't'),\n",
       "  ('e', 'e'),\n",
       "  ('n', 'n'),\n",
       "  ('t', 't'),\n",
       "  ('i', 'i'),\n",
       "  ('o', 'o'),\n",
       "  ('n', 'n')]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g', 't'),\n",
       " ('h', 'h'),\n",
       " ('e', 'e'),\n",
       " ('n', 'n'),\n",
       " ('c', 'c'),\n",
       " ('o', 'o'),\n",
       " ('m', 'm'),\n",
       " ('e', 'e'),\n",
       " ('t', 't'),\n",
       " ('o', 'o')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(data_sent, predictions, unk=False):\n",
    "    test = data_sent\n",
    "    compt=0\n",
    "    if(unk== True):\n",
    "        for i in range(len(predictions)): \n",
    "            if predictions[i]== test[i] or predictions[i] == ('CHUNK',0) :   # 0.96 si on compte les CHUNK\n",
    "                compt= compt+1\n",
    "    else:   \n",
    "        for i in range(len(predictions)): \n",
    "            if predictions[i]== test[i]: #   0.86 si on ne compte compte les CHUNK\n",
    "                compt= compt+1\n",
    "\n",
    "    return compt/len(predictions)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions =  associate_word_tag(data_test , compt_mot_train,compt_tag_train.keys(), trans, obs, ini )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Précision sur le test set\n",
    "les résultats sont sensiblement les mêmes sur train_set, car le vocabulaire qui n'est pas >10 occurences est traité de la même facon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8498592055598826"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train20 test20\n",
    "result_test, test_set = predictions \n",
    "precision(test_set, result_test, unk=False)  # on ne compte pas les unk comme bon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.923224043715847"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train10 test10\n",
    "result_test, test_set = predictions \n",
    "precision(test_set, result_test, unk=False)  # on ne compte pas les unk comme bon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.923224043715847"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(test_set, result_test, unk=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultat viterbi 1ere ordre 7.6% d erreur pour train10 et test10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07677595628415301"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = 1-precision(test_set, result_test, unk=False)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultat viterbi 1ere ordre 15% d erreur pour train10 et test10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1501407944401174"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = 1-precision(test_set, result_test, unk=False)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.898224043715847"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_nothing(data_test):\n",
    "    acc= 0\n",
    "    total= 0\n",
    "    for mot in data_test:\n",
    "        for l1, l2 in mot:\n",
    "            if l1==l2:\n",
    "                acc= acc+1\n",
    "            total= total+1\n",
    "    return acc/total\n",
    "# un peu moin de 3% d'amelioration avec un viterbi du 1er ordre \n",
    "do_nothing(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En faisant rien 19.4% train20 test20   :   4.4% d'erreur en moins avec viterbi et hmm d'ordre 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19405667725121323"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.4 % d'amélioration avec viterbi au premier ordre \n",
    "1-do_nothing(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En faisant rien 10% train10 test10 :  :  2.4% d'erreur en moins avec viterbi et hmm d'ordre 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10177595628415304"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-do_nothing(data_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TC4-tp2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
